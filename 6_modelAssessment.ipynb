{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the 2nd stage of the model assessment. The 1st stage, the evaluation metrics, were created from the training process in the final training. For the 3rd stage, the qualitative analysis, individual samples that were created in this notebook were considered and analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define run name\n",
    "run_name = \"finalTraining_v1\"\n",
    "# run_name = \"MLPC-2048-StarCoderBase7B\"\n",
    "\n",
    "# define model for tokenizer\n",
    "model_name = \"codellama/CodeLlama-7b-hf\"\n",
    "# model_name = \"bigcode/starcoderbase-7b\"\n",
    "\n",
    "# dataset import folder\n",
    "export_folder = \"./dataset/\" + run_name + \"/\"\n",
    "\n",
    "# model save path\n",
    "model_save_path = \"./models/\" + run_name + \"/\"\n",
    "\n",
    "# model checkpoint path\n",
    "model_checkpoint_path = \"./checkpoints/\" + run_name + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test loading model and inference with that model\n",
    "\n",
    "# load quantization config for 4bit quantization -> must be same as training\n",
    "quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "# load model from model_save_path with quantization config\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_save_path, quantization_config=quantization_config, low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# add pad token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, tokenizer, prompt, seed=42, custom_eos_token=\"<END>\", max_length=400):\n",
    "    # set a seed for generation\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    input_ids = input_ids.to('cuda')\n",
    "    end_token_id = tokenizer.encode(custom_eos_token, add_special_tokens=False)[0]\n",
    "\n",
    "    output = model.generate(input_ids, eos_token_id=end_token_id, temperature=0.1, max_length=max_length, do_sample=True, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # delete <START> from generated text\n",
    "    generated_text = generated_text.replace(\"<START>\", \"\")\n",
    "    print(generated_text)\n",
    "    \n",
    "    # cleanup\n",
    "    del output\n",
    "    del input_ids\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEPRECATED SAMPLE GENERATOR 1 - provide input in the confidence levels ###\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # Encode the prompt\n",
    "# input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# # Initialize the output as the input\n",
    "# output = input_ids\n",
    "\n",
    "# # Loop until the end token is generated\n",
    "# while True:\n",
    "#     # Predict the probabilities of the next token\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(output)\n",
    "#     predictions = outputs.logits[:, -1, :]\n",
    "#     probabilities = F.softmax(predictions, dim=-1)\n",
    "\n",
    "#     # Get the top 5 tokens and their probabilities\n",
    "#     top_probs, top_token_ids = torch.topk(probabilities, 5)\n",
    "\n",
    "#     # Print the top 5 tokens and their probabilities\n",
    "#     previous_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#     print(f\"\\nPrevious text: {previous_text} add tokens:\")\n",
    "#     for i in range(5):\n",
    "#         token = tokenizer.decode(top_token_ids[0][i])\n",
    "#         prob = top_probs[0][i].item()\n",
    "#         print(f\"Token: {token}, Confidence: {prob}\")\n",
    "\n",
    "#     # Get the token with the highest probability\n",
    "#     max_prob, max_token_id = torch.max(probabilities, dim=-1)\n",
    "\n",
    "#     # Check if the confidence is over 50%\n",
    "#     if max_prob.item() < 0.50:\n",
    "#         break\n",
    "\n",
    "#     # Append the token to the output\n",
    "#     output = torch.cat([output, max_token_id.unsqueeze(0)], dim=-1)\n",
    "\n",
    "# # Decode the output\n",
    "# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEPRECATED SAMPLE GENERATOR 2 ###\n",
    "# def deprecated_sample_generator2(model, tokenizer, prompt, use_custom_eos=False, custom_eos_token=\"}\", max_length=3000, confidence_threshold=0.01):\n",
    "#     \"\"\" Generate code completions for a given prompt using the model and tokenizer.\n",
    "#     \"\"\"\n",
    "\n",
    "\n",
    "#     # Encode the prompt\n",
    "#     input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "#     # Initialize the output as the input\n",
    "#     output = input_ids\n",
    "    \n",
    "#     # Loop until the end token is generated or counter is at max_length\n",
    "#     for i in tqdm.tqdm(range(max_length)):\n",
    "#         # Predict the probabilities of the next token\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(output)\n",
    "#         predictions = outputs.logits[:, -1, :]\n",
    "#         probabilities = torch.nn.functional.softmax(predictions, dim=-1)\n",
    "\n",
    "#         # Get the token with the highest probability\n",
    "#         max_prob, max_token_id = torch.max(probabilities, dim=-1)\n",
    "\n",
    "#         # Check if the confidence is over the threshold\n",
    "#         if max_prob.item() < confidence_threshold:\n",
    "#             break\n",
    "\n",
    "#         # Append the token to the output\n",
    "#         output = torch.cat([output, max_token_id.unsqueeze(0)], dim=-1)\n",
    "\n",
    "#         if len(output[0]) > 3 + len(custom_eos_token):\n",
    "#             evtl_end = tokenizer.decode(output[0][-3:], skip_special_tokens=True)\n",
    "#             if use_custom_eos:\n",
    "#                 if custom_eos_token in evtl_end:\n",
    "#                     break\n",
    "#             # check for <EOS> in evtl_end\n",
    "#             if \"<EOS>\" in evtl_end:\n",
    "#                 break\n",
    "        \n",
    "#         # decode every 1000 iterations and print output\n",
    "#         if len(output[0]) % 50 == 0:\n",
    "#             # print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "#             print(\"Length of output: \", len(output[0]))\n",
    "#             print(\"Max prob: \", max_prob.item())\n",
    "#             print(\"Max token: \", max_token_id.item())\n",
    "#             print(\"Counter: \", i)\n",
    "#             print(\"\")\n",
    "\n",
    "        \n",
    "#     # Decode the output\n",
    "#     generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "#     # delete <START> from generated text\n",
    "#     generated_text = generated_text.replace(\"<START>\", \"\")\n",
    "\n",
    "#     print(generated_text)\n",
    "\n",
    "#     # cleanup\n",
    "#     del output\n",
    "#     del input_ids\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test generate_nitrox_json_fast_hope\n",
    "prompt_test = '<START> {'\n",
    "test_output_0 = generate_samples(model, tokenizer, prompt_test, seed=0)\n",
    "\n",
    "print(test_output_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multi_prompts(prompts, model, tokenizer, custom_eos_token=\"<END>\", \n",
    "max_length=6000, export_path=\"gen_json\"):\n",
    "    counter = 0\n",
    "    for prompt in tqdm.tqdm(prompts):\n",
    "        generated_text = generate_samples(model, tokenizer, prompt, custom_eos_token=custom_eos_token, max_length=max_length)\n",
    "\n",
    "        # save generated text as file named after the model Type\n",
    "        with open(export_path + \"/\" + str(counter) + \"_\" + prompt.split(\"nitrox.dlc.mirror.model.\")[1].split(\",\")[0].replace(\"\\\"\",\"\") + \".json\", \"w\") as f:\n",
    "            f.write(generated_text)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate JSON Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of prompts. Every prompt should be used 5 times for inference with a different seed.\n",
    "prompts = [\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.EntityModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.ValueObjectModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.AggregateRootModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.IdentityModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.EnumModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.DomainServiceModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.RepositoryModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.ApplicationServiceModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.DomainEventModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.DomainCommandModel\"',\n",
    "  '<START>',\n",
    "  '<START> { ',\n",
    "  '<START> { \"',\n",
    "  '<START> { \"@class\" ',\n",
    "  '<START> { \"@class\" : ',\n",
    "  '<START> { \"@class\" : \"nitrox.',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.',\n",
    "  '',\n",
    "]\n",
    "\n",
    "prompts = 5 * prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export path\n",
    "export_path = \"gen_json\"\n",
    "\n",
    "# define the max length of the generated json samples. For the 11GB VRAM 2080 Ti, the max token length is 4000.\n",
    "max_token_length = 4000\n",
    "\n",
    "# defines the custom end token\n",
    "eos_token=\"<END>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate generate_multi_prompts\n",
    "generate_multi_prompts(prompts, model, tokenizer, custom_eos_token=eos_token, \n",
    "max_length=max_token_length, export_path=export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsibility Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_json(json_str):\n",
    "    \"\"\" Close a json string with missing closing brackets\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "\n",
    "    for char in json_str:\n",
    "        if char in '{[':\n",
    "            stack.append(char)\n",
    "        elif char in '}]':\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "\n",
    "    while stack:\n",
    "        char = stack.pop()\n",
    "        if char == '{':\n",
    "            json_str += '}'\n",
    "        elif char == '[':\n",
    "            json_str += ']'\n",
    "\n",
    "    return json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_json(json_file):\n",
    "    \"\"\"\n",
    "    Postprocessing function to complete and clean the JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. check the end of the JSON and delete the last uncomplete key/value pair. (Delete till the first komma \",\")\n",
    "    json_file = json_file[:json_file.rfind(\",\")]\n",
    "\n",
    "    # 2. create a array with the unclosed brackets in the json -> move sequential through the json and add when brackets are open and delete when they are closed. ((, {, [) -> (,), }, ])\n",
    "    json_file = close_json(json_file)\n",
    "   \n",
    "    # 3. replace double quotes\n",
    "    json_file = json_file.replace('\"\"', '\"')\n",
    "\n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all json_files in gen_json folder\n",
    "import os\n",
    "\n",
    "json_files = []\n",
    "for file in os.listdir(export_path):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(export_path + \"/\" + file, \"r\") as f:\n",
    "            json_files.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocess all json files\n",
    "completed_jsons = []\n",
    "for json_file in json_files:\n",
    "    completed_jsons.append(complete_json(json_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all jsons to gen_json/completed folder\n",
    "for i in range(len(completed_jsons)):\n",
    "    with open(export_path + \"/completed/\" + str(i) + \".json\", \"w\") as f:\n",
    "        f.write(complete_json(completed_jsons[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Processed JSON Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse json with json.loads and look for errors\n",
    "def parse_json(test_json):\n",
    "    try:\n",
    "        json.loads(test_json)\n",
    "        return \"No error\"\n",
    "    except Exception as e:\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all jsons in gen_json/completed folder\n",
    "jsons_to_parse = []\n",
    "path_to_json = []\n",
    "for file in os.listdir(export_path + \"/completed\"):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(export_path +\"/completed/\" + file, \"r\") as f:\n",
    "            jsons_to_parse.append(f.read())\n",
    "            path_to_json.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test all jsons\n",
    "results = {}\n",
    "for number, json_file in enumerate(jsons_to_parse):\n",
    "    # append results to results dict\n",
    "    results[number] = export_path(json_file)\n",
    "\n",
    "# save results to results.txt\n",
    "# with open(json_folder + \"/00_results.txt\", \"w\") as f:\n",
    "with open(export_path + \"/parsibility_results.txt\", \"w\") as f:\n",
    "    for key in results:\n",
    "        f.write(str(key) + \": \" + str(results[key]) + \"; \" + path_to_json[key] + \"\\n\")\n",
    "\n",
    "# write the results in a csv file\n",
    "import csv\n",
    "with open(export_path + \"/parsibility_results.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for key in results:\n",
    "        writer.writerow([key, results[key], path_to_json[key]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the samples pasibility are saved in the following files:\n",
    "- [gen_json/parsibility_results.csv](gen_json/parsibility_results.csv)\n",
    "- [gen_json/parsibility_results.txt](gen_json/parsibility_results.txt)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
