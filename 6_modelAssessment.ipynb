{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the 2nd stage of the model assessment. The 1st stage, the evaluation metrics, were created from the training process in the final training. For the 3rd stage, the qualitative analysis, individual samples that were created in this notebook were considered and analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define run name\n",
    "run_name = \"finalTraining_v1\"\n",
    "# run_name = \"MLPC-2048-StarCoderBase7B\"\n",
    "\n",
    "# define model for tokenizer\n",
    "model_name = \"codellama/CodeLlama-7b-hf\"\n",
    "# model_name = \"bigcode/starcoderbase-7b\"\n",
    "\n",
    "# dataset import folder\n",
    "export_folder = \"./dataset/\" + run_name + \"/\"\n",
    "\n",
    "# model save path\n",
    "model_save_path = \"./models/\" + run_name + \"/\"\n",
    "\n",
    "# model checkpoint path\n",
    "model_checkpoint_path = \"./checkpoints/\" + run_name + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test loading model and inference with that model\n",
    "\n",
    "# load quantization config for 4bit quantization -> must be same as training\n",
    "quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "# load model from model_save_path with quantization config\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_save_path, quantization_config=quantization_config, low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# add pad token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, tokenizer, prompt, seed=42, custom_eos_token=\"<END>\", max_length=400):\n",
    "    # set a seed for generation\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    input_ids = input_ids.to('cuda')\n",
    "    end_token_id = tokenizer.encode(custom_eos_token, add_special_tokens=False)[0]\n",
    "\n",
    "    output = model.generate(input_ids, eos_token_id=end_token_id, temperature=0.1, max_length=max_length, do_sample=True, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # delete <START> from generated text\n",
    "    generated_text = generated_text.replace(\"<START>\", \"\")\n",
    "    print(generated_text)\n",
    "    \n",
    "    # cleanup\n",
    "    del output\n",
    "    del input_ids\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test generate_nitrox_json_fast_hope\n",
    "prompt_test = '<START> {'\n",
    "test_output_0 = generate_samples(model, tokenizer, prompt_test, seed=0)\n",
    "\n",
    "print(test_output_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multi_prompts(prompts, model, tokenizer, custom_eos_token=\"<END>\", \n",
    "max_length=6000, export_path=\"gen_json\"):\n",
    "    counter = 0\n",
    "    for prompt in tqdm.tqdm(prompts):\n",
    "        generated_text = generate_samples(model, tokenizer, prompt, custom_eos_token=custom_eos_token, max_length=max_length)\n",
    "\n",
    "        # save generated text as file named after the model Type\n",
    "        with open(export_path + \"/\" + str(counter) + \"_\" + prompt.split(\"nitrox.dlc.mirror.model.\")[1].split(\",\")[0].replace(\"\\\"\",\"\") + \".json\", \"w\") as f:\n",
    "            f.write(generated_text)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate JSON Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of prompts. Every prompt should be used 5 times for inference with a different seed.\n",
    "prompts = [\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.EntityModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.ValueObjectModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.AggregateRootModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.IdentityModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.EnumModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.DomainServiceModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.RepositoryModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.ApplicationServiceModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.DomainEventModel\"',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.DomainCommandModel\"',\n",
    "  '<START>',\n",
    "  '<START> { ',\n",
    "  '<START> { \"',\n",
    "  '<START> { \"@class\" ',\n",
    "  '<START> { \"@class\" : ',\n",
    "  '<START> { \"@class\" : \"nitrox.',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.',\n",
    "  '<START> { \"@class\" : \"nitrox.dlc.mirror.model.',\n",
    "  '',\n",
    "]\n",
    "\n",
    "prompts = 5 * prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export path\n",
    "export_path = \"gen_json\"\n",
    "\n",
    "# define the max length of the generated json samples. For the 11GB VRAM 2080 Ti, the max token length is 4000.\n",
    "max_token_length = 4000\n",
    "\n",
    "# defines the custom end token\n",
    "eos_token=\"<END>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate generate_multi_prompts\n",
    "generate_multi_prompts(prompts, model, tokenizer, custom_eos_token=eos_token, \n",
    "max_length=max_token_length, export_path=export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsibility Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_json(json_str):\n",
    "    \"\"\" Close a json string with missing closing brackets\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "\n",
    "    for char in json_str:\n",
    "        if char in '{[':\n",
    "            stack.append(char)\n",
    "        elif char in '}]':\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "\n",
    "    while stack:\n",
    "        char = stack.pop()\n",
    "        if char == '{':\n",
    "            json_str += '}'\n",
    "        elif char == '[':\n",
    "            json_str += ']'\n",
    "\n",
    "    return json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_json(json_file):\n",
    "    \"\"\"\n",
    "    Postprocessing function to complete and clean the JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. check the end of the JSON and delete the last uncomplete key/value pair. (Delete till the first komma \",\")\n",
    "    json_file = json_file[:json_file.rfind(\",\")]\n",
    "\n",
    "    # 2. create a array with the unclosed brackets in the json -> move sequential through the json and add when brackets are open and delete when they are closed. ((, {, [) -> (,), }, ])\n",
    "    json_file = close_json(json_file)\n",
    "   \n",
    "    # 3. replace double quotes\n",
    "    json_file = json_file.replace('\"\"', '\"')\n",
    "\n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all json_files in gen_json folder\n",
    "import os\n",
    "\n",
    "json_files = []\n",
    "for file in os.listdir(export_path):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(export_path + \"/\" + file, \"r\") as f:\n",
    "            json_files.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocess all json files\n",
    "completed_jsons = []\n",
    "for json_file in json_files:\n",
    "    completed_jsons.append(complete_json(json_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all jsons to gen_json/completed folder\n",
    "for i in range(len(completed_jsons)):\n",
    "    with open(export_path + \"/completed/\" + str(i) + \".json\", \"w\") as f:\n",
    "        f.write(complete_json(completed_jsons[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Processed JSON Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse json with json.loads and look for errors\n",
    "def parse_json(test_json):\n",
    "    try:\n",
    "        json.loads(test_json)\n",
    "        return \"No error\"\n",
    "    except Exception as e:\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all jsons in gen_json/completed folder\n",
    "\n",
    "jsons_to_parse = []\n",
    "for file in os.listdir(export_path + \"/completed\"):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(export_path +\"/completed/\" + file, \"r\") as f:\n",
    "            jsons_to_parse.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test all jsons\n",
    "results = {}\n",
    "for number, json_file in enumerate(jsons_to_parse):\n",
    "    # append results to results dict\n",
    "    results[number] = parse_json(json_file)\n",
    "\n",
    "# save results to results.txt\n",
    "with open(export_path + \"/completed/results.txt\", \"w\") as f:\n",
    "    for key in results:\n",
    "        f.write(str(key) + \": \" + str(results[key]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
