{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import peft\n",
    "# from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define run name\n",
    "run_name = \"finalTraining_v1\"\n",
    "# run_name = \"MLPC-2048-StarCoderBase7B\"\n",
    "\n",
    "# define model for tokenizer\n",
    "model_name = \"codellama/CodeLlama-7b-hf\"\n",
    "# model_name = \"bigcode/starcoderbase-7b\"\n",
    "\n",
    "# dataset import folder\n",
    "export_folder = \"./dataset/\" + run_name + \"/\"\n",
    "\n",
    "# model save path\n",
    "model_save_path = \"./models/\" + run_name + \"/\"\n",
    "\n",
    "# model checkpoint path\n",
    "model_checkpoint_path = \"./checkpoints/\" + run_name + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ccdff2b10d49f38322ae31d2b81d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Test loading model and inference with that model\n",
    "\n",
    "# load quantization config for 4bit quantization -> must be same as training\n",
    "quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "# load model from model_save_path with quantization config\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_save_path, quantization_config=quantization_config, low_cpu_mem_usage=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# add pad token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)  # Set mlm=False for causal language modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.load_from_disk(export_folder + \"test_dataset\")\n",
    "\n",
    "dataset = datasets.load_from_disk(export_folder + \"train_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak. \n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    # select only the first element of the tuple when element is a tuple\n",
    "    if type(logits)==tuple:\n",
    "        logits = logits[0]\n",
    "\n",
    "    # select the argmax of the logits\n",
    "    logits = logits.argmax(axis=-1)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def decode_logits_labels(logits, labels, print_debugg=False):\n",
    "    '''Decode logits and labels to text and cut them to the same length using the decoder.\n",
    "    Set print_debugg to True to enable print outputs.\n",
    "    '''\n",
    "    ## Convert logits to text\n",
    "    predicted_text = [tokenizer.decode(logit[logit < tokenizer.vocab_size], skip_special_tokens=True) for logit in logits]\n",
    "\n",
    "    # concatenate predicted text to one string\n",
    "    predicted_text = ''.join(predicted_text)\n",
    "\n",
    "    ## Convert labels to text\n",
    "    # Labels shape: (16, 512)\n",
    "    label_text = [tokenizer.decode(label[label < tokenizer.vocab_size], skip_special_tokens=True) for label in labels]\n",
    "    \n",
    "    # concatenate label text to one string\n",
    "    label_text = ''.join(label_text)\n",
    "\n",
    "    # cut both to same length\n",
    "    predicted_text = predicted_text[:len(label_text)]\n",
    "    label_text = label_text[:len(predicted_text)]\n",
    "\n",
    "    if print_debugg:\n",
    "        print(\"ðŸ› ï¸ DEBUGG decode_logits_labels ðŸ› ï¸\")\n",
    "        print(f\"PREDICTED: {predicted_text}\")\n",
    "        print(f\"LABEL: {label_text}\")\n",
    "\n",
    "    return predicted_text, label_text\n",
    "\n",
    "def calcuate_rouge_in_compute_metrics(predicted_text, label_text, return_long_form=False, print_debugg=False):\n",
    "    ''' Calculate rouge score for a given model and predicted text in the compute metrics function.\n",
    "    '''\n",
    "\n",
    "    # compute rouge score\n",
    "    rouge = evaluate.load('rouge')\n",
    "\n",
    "    scores = rouge.compute(predictions=predicted_text, references=label_text)\n",
    "\n",
    "\n",
    "    if print_debugg:\n",
    "        print(\"ðŸ› ï¸ DEBUGG calcuate_rouge_in_compute_metrics ðŸ› ï¸\")\n",
    "\n",
    "        print(f\"Rouge scores: {scores}\")\n",
    "        # {'rouge1': 0.04678232172323061, 'rouge2': 0.0, 'rougeL': 0.04687151585425679, 'rougeLsum': 0.04680462025598715}\n",
    "    if return_long_form:\n",
    "        return {\"Rouge-1 f1\": scores['rouge1'], \"Rouge-2 f1\": scores['rouge2'], \"Rouge-L f1\": scores['rougeL'], \"Rouge-Lsum f1\": scores['rougeLsum']}\n",
    "    else:\n",
    "        return {\"rouge-lsum-f1\": scores['rougeLsum']}\n",
    "\n",
    "\n",
    "def calculate_bleu_score(predicted_text, label_text, return_long_form=False, print_debugg=False):\n",
    "    ''' Calculate bleu score for a given model and predicted text in the compute metrics function.\n",
    "    '''\n",
    "\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    bleu_score = bleu.compute(predictions=[predicted_text], references=[[label_text]])\n",
    "\n",
    "    if print_debugg:\n",
    "        print(\"ðŸ› ï¸ DEBUGG calculate_bleu_score ðŸ› ï¸\")\n",
    "        print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "    \n",
    "    if return_long_form:\n",
    "        return bleu_score\n",
    "    else:\n",
    "        return {\"BLEU Score\": bleu_score['bleu']}\n",
    "\n",
    "def calculate_perplexity_in_compute_metrics(model, predicted_text, label_text, return_long_form=False, print_debugg=False):\n",
    "    ''' Calculate perplexity for a given model and predicted text in the compute metrics function.\n",
    "    Hint: The model gets loaded every eval step and is also not loaded in quantized mode. -> Leads to high memory usage, slow evaluation times and unusable results when comparing it to the quantized model.\n",
    "    '''\n",
    "\n",
    "    # load metric from evalaute\n",
    "    perplexity = evaluate.load('perplexity', module_type=\"metric\")\n",
    "    \n",
    "    # compute perplexity\n",
    "    scores = perplexity.compute(model_id=model, predictions=predicted_text)\n",
    "    \n",
    "    if print_debugg:\n",
    "        print(\"ðŸ› ï¸ DEBUGG calculate_perplexity_in_compute_metrics ðŸ› ï¸\")\n",
    "        print(f\"Perplexity: {scores}\")\n",
    "    \n",
    "    return {\"Perplexity\": scores}\n",
    "\n",
    "\n",
    "def calculate_f1_score_in_compute_metrics(predicted_text, label_text, return_long_form=False, print_debugg=False):\n",
    "    ''' Calculate f1 score for a given model and predicted text in the compute metrics function.\n",
    "    Hint: The metric does not work yet, because the metric is not loaded correctly.\n",
    "    '''\n",
    "\n",
    "    # load metric from evalaute\n",
    "    f1_score = evaluate.load('f1', module_type=\"metric\")\n",
    "\n",
    "    # compute f1 score\n",
    "    scores_macro = f1_score.compute(predictions=predicted_text, references=label_text, average=\"macro\")\n",
    "    scores_micro = f1_score.compute(predictions=predicted_text, references=label_text, average=\"micro\")\n",
    "    scores_weighted = f1_score.compute(predictions=predicted_text, references=label_text, average=\"weighted\")\n",
    "\n",
    "\n",
    "    if print_debugg:\n",
    "        print(\"ðŸ› ï¸ DEBUGG calculate_f1_score_in_compute_metrics ðŸ› ï¸\")\n",
    "        print(f\"F1 Score macro: {scores_macro}\")\n",
    "        print(f\"F1 Score micro: {scores_micro}\")\n",
    "        print(f\"F1 Score weighted: {scores_weighted}\")\n",
    "\n",
    "    return {\"f1 macro\":scores_macro['f1'], \"f1 micro\":scores_micro['f1'], \"f1 weighted\":scores_weighted['f1']}\n",
    "\n",
    "\n",
    "def print_shape_logits_labels(logits, labels):\n",
    "    '''Print shape and dtype of logits and labels.'''\n",
    "    print(\"Logits shape:\", logits.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    print(\"Logits dtype:\", logits.dtype)\n",
    "    print(\"Labels dtype:\", labels.dtype)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # set debugg to true to enable all print outputs\n",
    "    debugg = False\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    if debugg:\n",
    "        print(\"ðŸ› ï¸ DEBUGG print shapes of logits and labels befor conversion ðŸ› ï¸\")\n",
    "        print_shape_logits_labels(logits, labels)\n",
    "\n",
    "    # mask -100 tokens from labels\n",
    "    mask = labels!=-100\n",
    "    logits, labels = logits[mask], labels[mask]\n",
    "\n",
    "    if debugg:\n",
    "        print(\"ðŸ› ï¸ DEBUGG print shapes of logits and labels after conversion ðŸ› ï¸\")\n",
    "        print_shape_logits_labels(logits, labels)\n",
    "\n",
    "    predicted_text, label_text =  decode_logits_labels(logits, labels, print_debugg=debugg)\n",
    "\n",
    "    eval_metrics = {}\n",
    "\n",
    "    # calculate rouge score\n",
    "    rouge_scores = calcuate_rouge_in_compute_metrics(predicted_text, label_text, return_long_form=True, print_debugg=debugg)\n",
    "    eval_metrics.update(rouge_scores)\n",
    "\n",
    "    # calculate bleu score\n",
    "    bleu_score = calculate_bleu_score(predicted_text, label_text, return_long_form=True, print_debugg=debugg)\n",
    "    eval_metrics.update(bleu_score)\n",
    "\n",
    "    # calculate f1 score\n",
    "    f1_score = calculate_f1_score_in_compute_metrics(logits, labels, return_long_form=True, print_debugg=debugg)\n",
    "    eval_metrics.update(f1_score)\n",
    "\n",
    "    # calculate perplexity -> Dont use this, it does not work properly. See Hint in function description.\n",
    "    # perplexity = calculate_perplexity_in_compute_metrics(model=model_name, predicted_text=predicted_text, label_text=label_text, return_long_form=True, print_debugg=True)\n",
    "    # eval_metrics.update(perplexity)\n",
    "   \n",
    "    return eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define model for tokenizer\n",
    "model_name = \"codellama/CodeLlama-7b-hf\"\n",
    "\n",
    "# set max length for dataset\n",
    "max_length = 2048\n",
    "# max_length = 16000 # 16k tokens from paper -> https://arxiv.org/pdf/2308.12950.pdf -> needs to much memory\n",
    "\n",
    "# set random seed for dataset shuffling\n",
    "rand_seed = 42\n",
    "\n",
    "# set export options\n",
    "save_dataset = True\n",
    "save_df = True\n",
    "\n",
    "# dataset import folder\n",
    "export_folder = \"./dataset/\" + \"finalTraining_v1\" + \"/\"\n",
    "\n",
    "## training Paths\n",
    "\n",
    "# Tensorboard folder\n",
    "tensorboard_logdir = \"./runs\"\n",
    "\n",
    "# model save path\n",
    "model_save_path = \"./models/\" + run_name + \"/\"\n",
    "\n",
    "# model checkpoint path\n",
    "model_checkpoint_path = \"./checkpoints/\" + run_name + \"/\"\n",
    "\n",
    "## Training parameters\n",
    "\n",
    "# set batch size per device\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# set number of gradient accumulation steps -> number of updates steps to accumulate before performing a backward/update pass\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# create model checkpoint every x steps\n",
    "save_steps=50\n",
    "\n",
    "# Keep keep last x checkpoints\n",
    "save_total_limit=100\n",
    "\n",
    "# Enable mixed precision training -> hugh enabler for low VRAM training\n",
    "fp16=True\n",
    "\n",
    "# Log every x steps\n",
    "logging_steps=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lora\n",
    "# lora rank\n",
    "lora_r_value = 10\n",
    "\n",
    "# lora alpha\n",
    "lora_alpha_value = 30\n",
    "\n",
    "# dropout for lora weights\n",
    "lora_dropout = 0.05\n",
    "\n",
    "\n",
    "## trainer\n",
    "# Number of warmup steps for learning rate scheduler\n",
    "warmup_steps=448\n",
    "\n",
    "# set number of epochs\n",
    "num_train_epochs = 16\n",
    "\n",
    "# Learning rate\n",
    "learning_rate=3.4e-5\n",
    "\n",
    "# %%\n",
    "### EVALUATION HYPERPARAMETERS ###\n",
    "\n",
    "eval_steps = 50\n",
    "\n",
    "per_device_eval_batch_size=1\n",
    "\n",
    "gradient_checkpointing=True\n",
    "\n",
    "eval_accumulation_steps=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "        output_dir=model_checkpoint_path,  # Output directory for model predictions and checkpoints\n",
    "        overwrite_output_dir=True,  # Overwrite existing output\n",
    "        num_train_epochs=num_train_epochs, # Number of training epochs\n",
    "        per_device_train_batch_size=per_device_train_batch_size,  # Batch size per device during training\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,  # Number of updates steps to accumulate before performing a backward/update pass\n",
    "        # save_steps=save_steps,  # Create model checkpoint every x steps\n",
    "        # save_total_limit=save_total_limit,  # Keep keep last x checkpoints\n",
    "        fp16=True,  # Enable mixed precision training -> hugh enabler for low VRAM training\n",
    "        # logging_dir=tensorboard_logdir,  # Directory for storing logs\n",
    "        # logging_steps=logging_steps,  # Log every x steps\n",
    "        warmup_steps=warmup_steps,  # Number of warmup steps for learning rate scheduler\n",
    "        learning_rate=learning_rate,  # Learning rate\n",
    "        evaluation_strategy=\"steps\",  # Evaluate every `logging_steps`\n",
    "        eval_steps=eval_steps,  # Evaluate every x steps\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,  # Batch size per device during evaluation\n",
    "        gradient_checkpointing=gradient_checkpointing,  # Enable gradient checkpointing to save memory\n",
    "        eval_accumulation_steps=eval_accumulation_steps,  # Accumulate evaluation steps\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='601' max='601' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [601/601 08:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9969825042969652, 0.993626161491241, 0.9901426125176636, 0.9867039499070591]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.030932707712054253, 'eval_Rouge-1 f1': 0.056495011311288285, 'eval_Rouge-2 f1': 0.0, 'eval_Rouge-L f1': 0.05649783208143375, 'eval_Rouge-Lsum f1': 0.05648827280482966, 'eval_bleu': 0.9918563846595103, 'eval_precisions': [0.9969825042969652, 0.993626161491241, 0.9901426125176636, 0.9867039499070591], 'eval_brevity_penalty': 1.0, 'eval_length_ratio': 1.000561485916329, 'eval_translation_length': 935544, 'eval_reference_length': 935019, 'eval_f1 macro': 0.0007592371021967591, 'eval_f1 micro': 2.071808896347401e-05, 'eval_f1 weighted': 1.9867474948267826e-05, 'eval_runtime': 1479.0957, 'eval_samples_per_second': 0.406, 'eval_steps_per_second': 0.406}\n"
     ]
    }
   ],
   "source": [
    "test_results = eval_trainer.evaluate()\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.030932707712054253, 'eval_Rouge-1 f1': 0.056495011311288285, 'eval_Rouge-2 f1': 0.0, 'eval_Rouge-L f1': 0.05649783208143375, 'eval_Rouge-Lsum f1': 0.05648827280482966, 'eval_bleu': 0.9918563846595103, 'eval_precisions': [0.9969825042969652, 0.993626161491241, 0.9901426125176636, 0.9867039499070591], 'eval_brevity_penalty': 1.0, 'eval_length_ratio': 1.000561485916329, 'eval_translation_length': 935544, 'eval_reference_length': 935019, 'eval_f1 macro': 0.0007592371021967591, 'eval_f1 micro': 2.071808896347401e-05, 'eval_f1 weighted': 1.9867474948267826e-05, 'eval_runtime': 1479.0957, 'eval_samples_per_second': 0.406, 'eval_steps_per_second': 0.406}\n"
     ]
    }
   ],
   "source": [
    "# get results from test evaluation\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
