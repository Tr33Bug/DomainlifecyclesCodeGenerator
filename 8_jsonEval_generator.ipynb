{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "# !pip install pandas\n",
    "# !pip3 install torch torchvision torchaudio\n",
    "# !pip install ipywidgets\n",
    "# !pip install bitsandbytes\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define run name\n",
    "run_name = \"finalTraining_v1\"\n",
    "# run_name = \"MLPC-2048-StarCoderBase7B\"\n",
    "\n",
    "# define model for tokenizer\n",
    "model_name = \"codellama/CodeLlama-7b-hf\"\n",
    "# model_name = \"bigcode/starcoderbase-7b\"\n",
    "\n",
    "# dataset import folder\n",
    "export_folder = \"./dataset/\" + run_name + \"/\"\n",
    "\n",
    "# model save path\n",
    "model_save_path = \"./models/\" + run_name + \"/\"\n",
    "\n",
    "# model checkpoint path\n",
    "model_checkpoint_path = \"./checkpoints/\" + run_name + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53898799c2e7473a84efbded671d87b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 10.58 GiB of which 16.44 MiB is free. Process 1665360 has 8.02 GiB memory in use. Including non-PyTorch memory, this process has 2.53 GiB memory in use. Of the allocated memory 2.21 GiB is allocated by PyTorch, and 154.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mBitsAndBytesConfig(load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m, bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# load model from model_save_path with quantization config\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# optional: load model untrained\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# model = transformers.AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, low_cpu_mem_usage=True)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# optional: load model from checkpoint\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# model = transformers.AutoModelForCausalLM.from_pretrained(\"./output/bigRun/checkpoint-1000\", quantization_config=quantization_config, low_cpu_mem_usage=True)\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/thesis-localllm-codetuning/FixCUDAShit/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m~/thesis/thesis-localllm-codetuning/FixCUDAShit/lib/python3.10/site-packages/transformers/modeling_utils.py:3502\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3494\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3495\u001b[0m     (\n\u001b[1;32m   3496\u001b[0m         model,\n\u001b[1;32m   3497\u001b[0m         missing_keys,\n\u001b[1;32m   3498\u001b[0m         unexpected_keys,\n\u001b[1;32m   3499\u001b[0m         mismatched_keys,\n\u001b[1;32m   3500\u001b[0m         offload_index,\n\u001b[1;32m   3501\u001b[0m         error_msgs,\n\u001b[0;32m-> 3502\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3509\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3510\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3514\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3521\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/thesis/thesis-localllm-codetuning/FixCUDAShit/lib/python3.10/site-packages/transformers/modeling_utils.py:3926\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3924\u001b[0m                     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, state_dict)\n\u001b[1;32m   3925\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3926\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3927\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3930\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3933\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3934\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3935\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3936\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3937\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3938\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3939\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3940\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3941\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3942\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3943\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   3944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/thesis/thesis-localllm-codetuning/FixCUDAShit/lib/python3.10/site-packages/transformers/modeling_utils.py:807\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    805\u001b[0m         set_module_tensor_to_device(model, param_name, param_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mset_module_kwargs)\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 807\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_quantized_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;66;03m# TODO: consider removing used param_parts from state_dict before return\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, offload_index, state_dict_index\n",
      "File \u001b[0;32m~/thesis/thesis-localllm-codetuning/FixCUDAShit/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:213\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.create_quantized_param\u001b[0;34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[0m\n\u001b[1;32m    210\u001b[0m         new_value \u001b[38;5;241m=\u001b[39m new_value\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    212\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m old_value\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m--> 213\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParams4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m new_value\n",
      "File \u001b[0;32m~/thesis/thesis-localllm-codetuning/FixCUDAShit/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:191\u001b[0m, in \u001b[0;36mParams4bit.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/thesis/thesis-localllm-codetuning/FixCUDAShit/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:169\u001b[0m, in \u001b[0;36mParams4bit.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m, device):\n\u001b[1;32m    168\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mhalf()\u001b[38;5;241m.\u001b[39mcuda(device)\n\u001b[0;32m--> 169\u001b[0m     w_4bit, quant_state \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompress_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress_statistics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m w_4bit\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_state \u001b[38;5;241m=\u001b[39m quant_state\n",
      "File \u001b[0;32m~/thesis/thesis-localllm-codetuning/FixCUDAShit/lib/python3.10/site-packages/bitsandbytes/functional.py:934\u001b[0m, in \u001b[0;36mquantize_4bit\u001b[0;34m(A, absmax, out, blocksize, compress_statistics, quant_type)\u001b[0m\n\u001b[1;32m    930\u001b[0m     absmax \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((blocks,), device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 934\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m blocksize \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m4096\u001b[39m, \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m]\n\u001b[1;32m    938\u001b[0m prev_device \u001b[38;5;241m=\u001b[39m pre_call(A\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 10.58 GiB of which 16.44 MiB is free. Process 1665360 has 8.02 GiB memory in use. Including non-PyTorch memory, this process has 2.53 GiB memory in use. Of the allocated memory 2.21 GiB is allocated by PyTorch, and 154.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "## Test loading model and inference with that model\n",
    "\n",
    "# load quantization config for 4bit quantization -> must be same as training\n",
    "quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "# load model from model_save_path with quantization config\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_save_path, quantization_config=quantization_config, low_cpu_mem_usage=True)\n",
    "\n",
    "# optional: load model untrained\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, low_cpu_mem_usage=True)\n",
    "\n",
    "# optional: load model unquantized and untrained\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True)\n",
    "\n",
    "# optional: load model from checkpoint\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"./output/bigRun/checkpoint-1000\", quantization_config=quantization_config, low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# add pad token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nitrox_json(model, tokenizer, prompt, use_custom_eos=False, custom_eos_token=\"}\", max_length=3000, confidence_threshold=0.01):\n",
    "    \"\"\" Generate code completions for a given prompt using the model and tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Initialize the output as the input\n",
    "    output = input_ids\n",
    "    \n",
    "    # Loop until the end token is generated or counter is at max_length\n",
    "    for i in tqdm.tqdm(range(max_length)):\n",
    "        # Predict the probabilities of the next token\n",
    "        with torch.no_grad():\n",
    "            outputs = model(output)\n",
    "        predictions = outputs.logits[:, -1, :]\n",
    "        probabilities = torch.nn.functional.softmax(predictions, dim=-1)\n",
    "\n",
    "        # Get the token with the highest probability\n",
    "        max_prob, max_token_id = torch.max(probabilities, dim=-1)\n",
    "\n",
    "        # Check if the confidence is over the threshold\n",
    "        if max_prob.item() < confidence_threshold:\n",
    "            break\n",
    "\n",
    "        # Append the token to the output\n",
    "        output = torch.cat([output, max_token_id.unsqueeze(0)], dim=-1)\n",
    "\n",
    "        if len(output[0]) > 3 + len(custom_eos_token):\n",
    "            evtl_end = tokenizer.decode(output[0][-3:], skip_special_tokens=True)\n",
    "            if use_custom_eos:\n",
    "                if custom_eos_token in evtl_end:\n",
    "                    break\n",
    "            # check for <EOS> in evtl_end\n",
    "            if \"<EOS>\" in evtl_end:\n",
    "                break\n",
    "        \n",
    "        # decode every 1000 iterations and print output\n",
    "        if len(output[0]) % 50 == 0:\n",
    "            # print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "            print(\"Length of output: \", len(output[0]))\n",
    "            print(\"Max prob: \", max_prob.item())\n",
    "            print(\"Max token: \", max_token_id.item())\n",
    "            print(\"Counter: \", i)\n",
    "            print(\"\")\n",
    "\n",
    "        \n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # delete <START> from generated text\n",
    "    generated_text = generated_text.replace(\"<START>\", \"\")\n",
    "\n",
    "    print(generated_text)\n",
    "\n",
    "    # cleanup\n",
    "    del output\n",
    "    del input_ids\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nitrox_json_fast_hope(model, tokenizer, prompt, seed=42, custom_eos_token=\"<END>\", max_length=400):\n",
    "    # set a seed for generation\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    input_ids = input_ids.to('cuda')\n",
    "    end_token_id = tokenizer.encode(custom_eos_token, add_special_tokens=False)[0]\n",
    "\n",
    "    output = model.generate(input_ids, eos_token_id=end_token_id, temperature=0.1, max_length=max_length, do_sample=True, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # delete <START> from generated text\n",
    "    generated_text = generated_text.replace(\"<START>\", \"\")\n",
    "    print(generated_text)\n",
    "    \n",
    "    # cleanup\n",
    "    del output\n",
    "    del input_ids\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:529 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:529 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " { \"typeName\": \"VAR_typeName\", \"domainType\": \"NON_DOMAIN\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []}}], \"returnType\": {\"@class\": \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"NON_DOMAIN\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []}, \"publishedEventTypeNames\": \"VAR_publishedEventTypeNames\", \"listenedEventTypeName\": null, \"getter\": false, \"setter\": false}, {\"@class\": \"nitrox.dlc.mirror.model.MethodModel\", \"name\": \"VAR_name\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"accessLevel\": \"PUBLIC\", \"parameters\": [], \"returnType\": {\"@class\": \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"NON_DOMAIN\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []},\n",
      " { \"typeName\": \"VAR_typeName\", \"domainType\": \"NON_DOMAIN\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []}}], \"returnType\": {\"@class\": \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"NON_DOMAIN\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []}, \"publishedEventTypeNames\": \"VAR_publishedEventTypeNames\", \"listenedEventTypeName\": null, \"getter\": false, \"setter\": false}, {\"@class\": \"nitrox.dlc.mirror.model.MethodModel\", \"name\": \"VAR_name\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"accessLevel\": \"PUBLIC\", \"parameters\": [], \"returnType\": {\"@class\": \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"NON_DOMAIN\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []},\n",
      " { \"typeName\": \"VAR_typeName\", \"domainType\": \"NON_DOMAIN\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []}}], \"returnType\": {\"@class\": \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"NON_DOMAIN\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []}, \"publishedEventTypeNames\": \"VAR_publishedEventTypeNames\", \"listenedEventTypeName\": null, \"getter\": false, \"setter\": false}, {\"@class\": \"nitrox.dlc.mirror.model.MethodModel\", \"name\": \"VAR_name\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"accessLevel\": \"PUBLIC\", \"parameters\": [], \"returnType\": {\"@class\": \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"NON_DOMAIN\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []},\n",
      " { \"typeName\": \"VAR_typeName\", \"domainType\": \"NON_DOMAIN\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []}}], \"returnType\": {\"@class\": \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"NON_DOMAIN\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []}, \"publishedEventTypeNames\": \"VAR_publishedEventTypeNames\", \"listenedEventTypeName\": null, \"getter\": false, \"setter\": false}, {\"@class\": \"nitrox.dlc.mirror.model.MethodModel\", \"name\": \"VAR_name\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"accessLevel\": \"PUBLIC\", \"parameters\": [], \"returnType\": {\"@class\": \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"NON_DOMAIN\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []},\n"
     ]
    }
   ],
   "source": [
    "# test generate_nitrox_json_fast_hope\n",
    "prompt_test = '<START> {'\n",
    "test_output_0 = generate_nitrox_json_fast_hope(model, tokenizer, prompt_test, seed=0)\n",
    "\n",
    "print(test_output_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multi_prompts(prompts, model, tokenizer, use_custom_eos=True, custom_eos_token='\"valueObject\" : true}', \n",
    "max_length=6000, confidence_threshold=0.01, export_path=\"gen_json\"):\n",
    "    counter = 0\n",
    "    for prompt in tqdm.tqdm(prompts):\n",
    "        # generated_text = generate_nitrox_json(model, tokenizer, prompt, use_custom_eos=use_custom_eos, custom_eos_token=custom_eos_token, max_length=max_length, confidence_threshold=confidence_threshold)\n",
    "        generated_text = generate_nitrox_json_fast_hope(model, tokenizer, prompt, custom_eos_token=custom_eos_token, max_length=max_length)\n",
    "\n",
    "        # save generated text as file named after the model Type\n",
    "        with open(export_path + \"/\" + str(counter) + \"_\" + prompt.split(\"nitrox.dlc.mirror.model.\")[1].split(\",\")[0].replace(\"\\\"\",\"\") + \".json\", \"w\") as f:\n",
    "            f.write(generated_text)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate from Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate JSON to max token length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    '<START> { \"@class\" : \"nitrox.dlc.mirror.model.EntityModel\"',\n",
    "    '<START> { \"@class\" : \"nitrox.dlc.mirror.model.ValueObjectModel\"',\n",
    "    '<START> { \"@class\" : \"nitrox.dlc.mirror.model.AggregateRootModel\"',\n",
    "    '<START> { \"@class\" : \"nitrox.dlc.mirror.model.IdentityModel\"',\n",
    "    '<START> { \"@class\" : \"nitrox.dlc.mirror.model.EnumModel\"',\n",
    "    '<START> { \"@class\" : \"nitrox.dlc.mirror.model.DomainServiceModel\"',\n",
    "    '<START> { \"@class\" : \"nitrox.dlc.mirror.model.RepositoryModel\"',\n",
    "    '<START> { \"@class\" : \"nitrox.dlc.mirror.model.ApplicationServiceModel\"',\n",
    "    '<START> { \"@class\" : \"nitrox.dlc.mirror.model.DomainEventModel\"',\n",
    "    '<START> { \"@class\" : \"nitrox.dlc.mirror.model.DomainCommandModel\"',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiply prompts for more prompts -> prompt = 10 * prompt\n",
    "prompts = 1 * prompts\n",
    "\n",
    "len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:529 for open-end generation.\n",
      " 10%|█         | 1/10 [00:17<02:40, 17.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:529 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " { \"@class\" : \"nitrox.dlc.mirror.model.EntityModel\" , \"typeName\" : \"VAR_typeName\" , \"abstract\" : false , \"allFields\" : [{\"@class\" : \"nitrox.dlc.mirror.model.ValueReferenceModel\", \"name\" : \"VAR_name\", \"type\": {\"@class\" : \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\" : \"VAR_typeName\", \"domainType\" : \"IDENTITY\", \"assertions\" : [{\"@class\" : \"nitrox.dlc.mirror.model.AssertionModel\", \"assertionType\" : \"isNotNull\", \"param1\" : null, \"param2\" : null, \"messageCode\" : \"VAR_messageCode\"}, {\"@class\" : \"nitrox.dlc.mirror.model.AssertionModel\", \"assertionType\" : \"isPositive\", \"param1\" : null, \"param2\" : null, \"messageCode\" : \"VAR_messageCode\"}], \"hasOptionalContainer\" : false, \"hasCollectionContainer\" : false, \"hasListContainer\" : false, \"hasSetContainer\" : false, \"hasStreamContainer\" : false, \"containerTypeName\" :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:35<02:21, 17.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:529 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " { \"@class\" : \"nitrox.dlc.mirror.model.ValueObjectModel\" , \"typeName\" : \"VAR_typeName\" , \"abstract\" : false , \"allFields\" : [{\"@class\" : \"nitrox.dlc.mirror.model.ValueReferenceModel\", \"name\" : \"VAR_name\", \"type\": {\"@class\" : \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\" : \"VAR_typeName\", \"domainType\" : \"IDENTITY\", \"assertions\" : [{\"@class\" : \"nitrox.dlc.mirror.model.AssertionModel\", \"assertionType\" : \"isNotNull\", \"param1\" : null, \"param2\" : null, \"message\" : \"{jakarta.validation.constraints.NotNull.message}\"}], \"hasOptionalContainer\" : false, \"hasCollectionContainer\" : false, \"hasListContainer\" : false, \"hasSetContainer\" : false, \"hasStreamContainer\" : false, \"containerTypeName\" : \"VAR_containerTypeName\", \"containerAssertions\" : []}, \"accessLevel\" : \"PRIVATE\", \"declaredByTypeName\" : \"VAR_declaredByTypeName\", \"modifiable\" :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:52<02:03, 17.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:529 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " { \"@class\" : \"nitrox.dlc.mirror.model.AggregateRootModel\" , \"typeName\" : \"VAR_typeName\" , \"abstract\" : false , \"allFields\" : [{\"@class\" : \"nitrox.dlc.mirror.model.ValueReferenceModel\", \"name\" : \"VAR_name\", \"type\": {\"@class\" : \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\" : \"VAR_typeName\", \"domainType\" : \"IDENTITY\", \"assertions\" : [{\"@class\" : \"nitrox.dlc.mirror.model.AssertionModel\", \"assertionType\" : \"isNotNull\", \"param1\" : null, \"param2\" : null, \"message\" : \"{jakarta.validation.constraints.NotNull.message}\"}], \"hasOptionalContainer\" : false, \"hasCollectionContainer\" : false, \"hasListContainer\" : false, \"hasSetContainer\" : false, \"hasStreamContainer\" : false, \"containerTypeName\" : \"VAR_containerTypeName\", \"containerAssertions\" : []}, \"accessLevel\" : \"PRIVATE\", \"declaredByTypeName\" : \"VAR_declaredByTypeName\", \"modifiable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:10<01:45, 17.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:529 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " { \"@class\" : \"nitrox.dlc.mirror.model.IdentityModel\" , \"typeName\" : \"VAR_typeName\", \"abstract\": false, \"allFields\": [{\"@class\": \"nitrox.dlc.mirror.model.FieldModel\", \"name\": \"VAR_name\", \"type\": {\"@class\": \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"NON_DOMAIN\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []}, \"accessLevel\": \"PRIVATE\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"modifiable\": false, \"publicReadable\": true, \"publicWriteable\": false, \"static\": false}], \"methods\": [{\"@class\": \"nitrox.dlc.mirror.model.MethodModel\", \"name\": \"VAR_name\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"accessLevel\": \"PUBLIC\", \"parameters\": [], \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:28<01:28, 17.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:529 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " { \"@class\" : \"nitrox.dlc.mirror.model.EnumModel\" , \"typeName\" : \"VAR_typeName\" , \"abstract\" : false , \"allFields\" : [{\"@class\" : \"nitrox.dlc.mirror.model.FieldModel\" , \"name\" : \"VAR_name\" , \"type\": {\"@class\" : \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"NON_DOMAIN\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []}, \"accessLevel\": \"PRIVATE\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"modifiable\": false, \"publicReadable\": true, \"publicWriteable\": false, \"static\": false}, {\"@class\" : \"nitrox.dlc.mirror.model.FieldModel\" , \"name\" : \"VAR_name\" , \"type\": {\"@class\" : \"nitrox.dlc.mirror.model.AssertedContain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:45<01:10, 17.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:529 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " { \"@class\" : \"nitrox.dlc.mirror.model.DomainServiceModel\" , \"typeName\" : \"VAR_typeName\" , \"abstract\": false, \"allFields\": [{\"@class\": \"nitrox.dlc.mirror.model.FieldModel\", \"name\": \"VAR_name\", \"type\": {\"@class\": \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"REPOSITORY\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []}, \"accessLevel\": \"PRIVATE\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"modifiable\": false, \"publicReadable\": false, \"publicWriteable\": false, \"static\": false}, {\"@class\": \"nitrox.dlc.mirror.model.FieldModel\", \"name\": \"VAR_name\", \"type\": {\"@class\": \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:03<00:52, 17.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:529 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " { \"@class\" : \"nitrox.dlc.mirror.model.RepositoryModel\" , \"typeName\" : \"VAR_typeName\" , \"abstract\" : false , \"allFields\" : [{\"@class\" : \"nitrox.dlc.mirror.model.FieldModel\", \"name\" : \"VAR_name\", \"type\": {\"@class\" : \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"NON_DOMAIN\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []}, \"accessLevel\": \"PRIVATE\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"modifiable\": false, \"publicReadable\": false, \"publicWriteable\": false, \"static\": false}, {\"@class\" : \"nitrox.dlc.mirror.model.FieldModel\", \"name\": \"VAR_name\", \"type\": {\"@class\": \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [02:21<00:35, 17.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:529 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " { \"@class\" : \"nitrox.dlc.mirror.model.ApplicationServiceModel\" , \"typeName\" : \"VAR_typeName\" , \"abstract\" : false , \"allFields\" : [{\"@class\" : \"nitrox.dlc.mirror.model.FieldModel\", \"name\" : \"VAR_name\", \"type\": {\"@class\" : \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"DOMAIN_SERVICE\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []}, \"accessLevel\": \"PRIVATE\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"modifiable\": false, \"publicReadable\": false, \"publicWriteable\": false, \"static\": false}, {\"@class\" : \"nitrox.dlc.mirror.model.FieldModel\", \"name\"\": \"VAR_name\", \"type\": {\"@class\" : \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:38<00:17, 17.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:529 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " { \"@class\" : \"nitrox.dlc.mirror.model.DomainEventModel\" , \"typeName\" : \"VAR_typeName\" , \"abstract\": false, \"allFields\": [{\"@class\": \"nitrox.dlc.mirror.model.ValueReferenceModel\", \"name\": \"VAR_name\", \"type\": {\"@class\": \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"IDENTITY\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []}, \"accessLevel\": \"PRIVATE\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"modifiable\": false, \"publicReadable\": true, \"publicWriteable\": false, \"static\": false}, {\"@class\": \"nitrox.dlc.mirror.model.ValueReferenceModel\", \"name\": \"VAR_name\", \"type\": {\"@class\": \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:56<00:00, 17.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " { \"@class\" : \"nitrox.dlc.mirror.model.DomainCommandModel\" , \"typeName\" : \"VAR_typeName\" , \"abstract\" : false , \"allFields\" : [{\"@class\" : \"nitrox.dlc.mirror.model.ValueReferenceModel\", \"name\" : \"VAR_name\", \"type\": {\"@class\" : \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"IDENTITY\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []}, \"accessLevel\": \"PRIVATE\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"modifiable\": false, \"publicReadable\": true, \"publicWriteable\": false, \"static\": false}, {\"@class\" : \"nitrox.dlc.mirror.model.ValueReferenceModel\", \"name\" : \"VAR_name\", \"type\": {\"@class\" : \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate generate_multi_prompts\n",
    "generate_multi_prompts(prompts, model, tokenizer, use_custom_eos=False, custom_eos_token=\"<END>\", max_length=300, confidence_threshold=0.01, export_path=\"gen_json_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_json(json_str):\n",
    "    stack = []\n",
    "\n",
    "    for char in json_str:\n",
    "        if char in '{[':\n",
    "            stack.append(char)\n",
    "        elif char in '}]':\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "\n",
    "    while stack:\n",
    "        char = stack.pop()\n",
    "        if char == '{':\n",
    "            json_str += '}'\n",
    "        elif char == '[':\n",
    "            json_str += ']'\n",
    "\n",
    "    return json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_double_quotes(json_str):\n",
    "    return json_str.replace('\"\"', '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_json(json_file):\n",
    "    \"\"\"\n",
    "    clean and close the json \n",
    "    \"\"\"\n",
    "\n",
    "    # 1. check the end of the JSON and delete the last uncomplete key/value pair. (Delete till the first komma \",\")\n",
    "    json_file = json_file[:json_file.rfind(\",\")]\n",
    "\n",
    "    # 2. create a array with the unclosed brackets in the json -> move sequential through the json and add when brackets are open and delete when they are closed. ((, {, [) -> (,), }, ])\n",
    "    open_brackets = []\n",
    "    closed_brackets = []\n",
    "    for char in json_file:\n",
    "        if char in [\"(\", \"{\", \"[\"]:\n",
    "            open_brackets.append(char)\n",
    "        if char in [\")\", \"}\", \"]\"]:\n",
    "            closed_brackets.append(char)\n",
    "    \n",
    "\n",
    "\n",
    "    json_file = close_json(json_file)\n",
    "    # find the missing brackets by comparing the two arrays\n",
    "    # brackets = []\n",
    "    # for bracket in open_brackets:\n",
    "    #     if bracket == \"(\":\n",
    "    #         if \")\" not in closed_brackets:\n",
    "    #             brackets.append(bracket)\n",
    "    #         else:\n",
    "    #             closed_brackets.remove(\")\")\n",
    "    #     elif bracket == \"{\":\n",
    "    #         if \"}\" not in closed_brackets:\n",
    "    #             brackets.append(bracket)\n",
    "    #         else:\n",
    "    #             closed_brackets.remove(\"}\")\n",
    "    #     elif bracket == \"[\":\n",
    "    #         if \"]\" not in closed_brackets:\n",
    "    #             brackets.append(bracket)\n",
    "    #         else:\n",
    "    #             closed_brackets.remove(\"]\")\n",
    "    \n",
    "    # print(brackets)\n",
    "    # # 3. add the missing brackets from the array to the end of the json\n",
    "    # for bracket in brackets:\n",
    "    #     if bracket == \"(\":\n",
    "    #         json_file += \")\"\n",
    "    #     elif bracket == \"{\":\n",
    "    #         json_file += \"}\"\n",
    "    #     elif bracket == \"[\":\n",
    "    #         json_file += \"]\"\n",
    "    \n",
    "    # 4. replace double quotes\n",
    "    json_file = replace_double_quotes(json_file)\n",
    "\n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder\n",
    "folder = \"gen_json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all json_files in gen_json folder\n",
    "import os\n",
    "\n",
    "json_files = []\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(folder + \"/\" + file, \"r\") as f:\n",
    "            json_files.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete all jsons\n",
    "completed_jsons = []\n",
    "for json_file in json_files:\n",
    "    completed_jsons.append(complete_json(json_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all jsons to gen_json/completed folder\n",
    "for i in range(len(completed_jsons)):\n",
    "    with open(folder + \"/completed/\" + str(i) + \".json\", \"w\") as f:\n",
    "        f.write(complete_json(completed_jsons[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse completed JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test test_json_comp with json.loads and look for errors\n",
    "def test_json_function(test_json):\n",
    "    try:\n",
    "        json.loads(test_json)\n",
    "        return \"No error\"\n",
    "    except Exception as e:\n",
    "        return e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No error'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_json = ' { \"@class\" : \"nitrox.dlc.mirror.model.ApplicationServiceModel\" , \"typeName\" : \"VAR_typeName\" }'\n",
    "\n",
    "result = test_json_function(test_json)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all jsons in gen_json/completed folder\n",
    "\n",
    "jsons_to_parse = []\n",
    "for file in os.listdir(folder + \"/completed\"):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(folder +\"/completed/\" + file, \"r\") as f:\n",
    "            jsons_to_parse.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test all jsons\n",
    "results = {}\n",
    "for number, json_file in enumerate(jsons_to_parse):\n",
    "    # append results to results dict\n",
    "    results[number] = test_json_function(json_file)\n",
    "\n",
    "# save results to results.txt\n",
    "with open(folder + \"/completed/results.txt\", \"w\") as f:\n",
    "    for key in results:\n",
    "        f.write(str(key) + \": \" + str(results[key]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
