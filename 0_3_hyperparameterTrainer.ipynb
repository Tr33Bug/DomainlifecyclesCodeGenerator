{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we cunduct a hyperparameter tuning to find the best hyperparameters for the given circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not already installed, install the following packages and restart the kernel.\n",
    "\n",
    "The use of the same python enviroment as for the previous notebooks is recommended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna\n",
    "# !pip install jupyterlab jupyterlab-optuna\n",
    "# !pip install optuna-dashboard\n",
    "# !pip install plotly\n",
    "# !pip install rouge_score\n",
    "# !pip install transformers\n",
    "# !pip install pandas\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import plotly\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define run name\n",
    "# use the same run name as in the previous notebooks. \n",
    "run_name = \"NCG_RUN_1\"\n",
    "\n",
    "# define model for tokenizer\n",
    "model_name = \"codellama/CodeLlama-7b-hf\"\n",
    "\n",
    "# set max length for dataset\n",
    "max_length = 512\n",
    "# max_length = 16000 # 16k tokens from paper -> https://arxiv.org/pdf/2308.12950.pdf -> needs to much memory\n",
    "\n",
    "# set random seed for dataset shuffling\n",
    "rand_seed = 42\n",
    "\n",
    "# set export options\n",
    "save_dataset = True\n",
    "save_df = True\n",
    "\n",
    "# dataset import folder\n",
    "export_folder = \"./dataset/\" + run_name + \"/\"\n",
    "\n",
    "## training Paths\n",
    "\n",
    "# Tensorboard folder\n",
    "tensorboard_logdir = \"./runs\"\n",
    "\n",
    "# model save path\n",
    "model_save_path = \"./models/\" + run_name + \"/\"\n",
    "\n",
    "# model checkpoint path\n",
    "model_checkpoint_path = \"./checkpoints/\" + run_name + \"/\"\n",
    "\n",
    "## Training parameters\n",
    "\n",
    "# set batch size per device\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# set number of gradient accumulation steps -> number of updates steps to accumulate before performing a backward/update pass\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# create model checkpoint every x steps\n",
    "save_steps=20\n",
    "\n",
    "# Keep keep last x checkpoints\n",
    "save_total_limit=5\n",
    "\n",
    "# Enable mixed precision training -> hugh enabler for low VRAM training\n",
    "fp16=True\n",
    "\n",
    "# Log every x steps\n",
    "logging_steps=50\n",
    "\n",
    "\n",
    "# dropout for lora weights\n",
    "lora_dropout = 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EVALUATION HYPERPARAMETERS ###\n",
    "\n",
    "# number of steps to evaluate\n",
    "eval_steps = 150\n",
    "\n",
    "# batch size for evaluation\n",
    "per_device_eval_batch_size=1\n",
    "\n",
    "# enable gradient checkpointing\n",
    "gradient_checkpointing=True\n",
    "\n",
    "# number of gradient accumulation steps for evaluation\n",
    "eval_accumulation_steps=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets from disk\n",
    "dataset = datasets.load_from_disk(export_folder + \"train_dataset\")\n",
    "eval_dataset = datasets.load_from_disk(export_folder + \"eval_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1921\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"input_ids\"][42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# add pad token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)  # Set mlm=False for causal language modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak. \n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    # select only the first element of the tuple when element is a tuple\n",
    "    if type(logits)==tuple:\n",
    "        logits = logits[0]\n",
    "\n",
    "    # select the argmax of the logits\n",
    "    logits = logits.argmax(axis=-1)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def decode_logits_labels(logits, labels):\n",
    "    '''Decode logits and labels to text and cut them to the same length using the decoder.\n",
    "    Set print_debugg to True to enable print outputs.\n",
    "    '''\n",
    "    ## Convert logits to text\n",
    "    predicted_text = [tokenizer.decode(logit[logit < tokenizer.vocab_size], skip_special_tokens=True) for logit in logits]\n",
    "\n",
    "    # concatenate predicted text to one string\n",
    "    predicted_text = ''.join(predicted_text)\n",
    "\n",
    "    ## Convert labels to text\n",
    "    # Labels shape: (16, 512)\n",
    "    label_text = [tokenizer.decode(label[label < tokenizer.vocab_size], skip_special_tokens=True) for label in labels]\n",
    "    \n",
    "    # concatenate label text to one string\n",
    "    label_text = ''.join(label_text)\n",
    "\n",
    "    # cut both to same length\n",
    "    predicted_text = predicted_text[:len(label_text)]\n",
    "    label_text = label_text[:len(predicted_text)]\n",
    "\n",
    "    return predicted_text, label_text\n",
    "\n",
    "def calcuate_rouge_in_compute_metrics(predicted_text, label_text, return_long_form=False):\n",
    "    ''' Calculate rouge score for a given model and predicted text in the compute metrics function.\n",
    "    '''\n",
    "\n",
    "    # compute rouge score\n",
    "    rouge = evaluate.load('rouge')\n",
    "\n",
    "    scores = rouge.compute(predictions=predicted_text, references=label_text)\n",
    "\n",
    "    if return_long_form:\n",
    "        return {\"Rouge-1 f1\": scores['rouge1'], \"Rouge-2 f1\": scores['rouge2'], \"Rouge-L f1\": scores['rougeL'], \"Rouge-Lsum f1\": scores['rougeLsum']}\n",
    "    else:\n",
    "        return {\"rouge-lsum-f1\": scores['rougeLsum']}\n",
    "\n",
    "\n",
    "def calculate_bleu_score(predicted_text, label_text, return_long_form=False):\n",
    "    ''' Calculate bleu score for a given model and predicted text in the compute metrics function.\n",
    "    '''\n",
    "\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    bleu_score = bleu.compute(predictions=[predicted_text], references=[[label_text]])\n",
    "\n",
    "    \n",
    "    if return_long_form:\n",
    "        return bleu_score\n",
    "    else:\n",
    "        return {\"BLEU Score\": bleu_score['bleu']}\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    # receive logits and labels from eval_pred\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "\n",
    "    # mask -100 tokens from labels\n",
    "    mask = labels!=-100\n",
    "    logits, labels = logits[mask], labels[mask]\n",
    "\n",
    "\n",
    "    predicted_text, label_text =  decode_logits_labels(logits, labels)\n",
    "\n",
    "    eval_metrics = {}\n",
    "\n",
    "    # calculate rouge score\n",
    "    rouge_scores = calcuate_rouge_in_compute_metrics(predicted_text, label_text, return_long_form=True)\n",
    "    eval_metrics.update(rouge_scores)\n",
    "\n",
    "    # calculate bleu score\n",
    "    bleu_score = calculate_bleu_score(predicted_text, label_text, return_long_form=True)\n",
    "    eval_metrics.update(bleu_score)\n",
    "\n",
    "   \n",
    "    return eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna Objective Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    ### define hyper parameters to tune\n",
    "    ## Lora\n",
    "    lora_r_value = trial.suggest_int(\"lora_r_value\", 4, 16)\n",
    "    lora_alpha_value = trial.suggest_int(\"lora_alpha_value\", 4, 16)\n",
    "\n",
    "    ## trainer\n",
    "    warmup_steps = trial.suggest_int(\"warmup_steps\", 200, 1200)\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 1, 5)\n",
    "    learning_rate= trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    \n",
    "    ## Load Model\n",
    "    # load quaNtization config for 4bit quantization\n",
    "    quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)  \n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, low_cpu_mem_usage=True)\n",
    "\n",
    "\n",
    "    # lora config quantisation params\n",
    "    lora_config = peft.LoraConfig(\n",
    "        r=lora_r_value,\n",
    "        lora_alpha=lora_alpha_value,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    model.add_adapter(lora_config)\n",
    "\n",
    "    ## Training Arguments\n",
    "    training_args = transformers.TrainingArguments(\n",
    "        output_dir=model_checkpoint_path,  # Output directory for model predictions and checkpoints\n",
    "        overwrite_output_dir=True,  # Overwrite existing output\n",
    "        num_train_epochs=num_train_epochs, # Number of training epochs\n",
    "        per_device_train_batch_size=per_device_train_batch_size,  # Batch size per device during training\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,  # Number of updates steps to accumulate before performing a backward/update pass\n",
    "        # save_steps=save_steps,  # Create model checkpoint every x steps\n",
    "        # save_total_limit=save_total_limit,  # Keep keep last x checkpoints\n",
    "        fp16=True,  # Enable mixed precision training -> hugh enabler for low VRAM training\n",
    "        # logging_dir=tensorboard_logdir,  # Directory for storing logs\n",
    "        # logging_steps=logging_steps,  # Log every x steps\n",
    "        warmup_steps=warmup_steps,  # Number of warmup steps for learning rate scheduler\n",
    "        learning_rate=learning_rate,  # Learning rate\n",
    "        evaluation_strategy=\"steps\",  # Evaluate every `logging_steps`\n",
    "        eval_steps=eval_steps,  # Evaluate every x steps\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,  # Batch size per device during evaluation\n",
    "        gradient_checkpointing=gradient_checkpointing,  # Enable gradient checkpointing to save memory\n",
    "        eval_accumulation_steps=eval_accumulation_steps,  # Accumulate evaluation steps\n",
    "    )\n",
    "\n",
    "    # define trainer for training \n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        # Try to not evaluate during training to speed up process\n",
    "        # compute_metrics=compute_metrics,\n",
    "        # preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    trainer.train()\n",
    "\n",
    "    eval_trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    )\n",
    "\n",
    "    # evaluate model\n",
    "    eval_results = eval_trainer.evaluate()\n",
    "    \n",
    "\n",
    "    # return eval results\n",
    "    return eval_results[\"eval_loss\"], eval_results[\"eval_bleu\"], eval_results[\"eval_Rouge-Lsum f1\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study Definition and Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_228419/3411567254.py:4: ExperimentalWarning: NSGAIIISampler is experimental (supported from v3.2.0). The interface can change in the future.\n",
      "  sampler = optuna.samplers.NSGAIIISampler(seed=rand_seed)\n"
     ]
    }
   ],
   "source": [
    "# define sampler\n",
    "# sampler = optuna.samplers.TPESampler(seed=rand_seed)\n",
    "## use NSGAIISampler sampler to optimize for multiple objectives\n",
    "sampler = optuna.samplers.NSGAIIISampler(seed=rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-09 12:35:02,395] A new study created in RDB with name: A6000_OptunaRun_2048-test\n"
     ]
    }
   ],
   "source": [
    "study_name = run_name\n",
    "storage_name = f\"sqlite:///optuna/{study_name}.db\"\n",
    "study = optuna.create_study(directions=['minimize', 'maximize', 'maximize'], study_name=study_name, storage=storage_name, sampler=sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Hyperparameter Tuning Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and Export Study Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Optuna Study Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna study\n",
    "study_name = run_name\n",
    "storage_name = f\"sqlite:///optuna/{study_name}.db\"\n",
    "study = optuna.load_study(study_name=study_name, storage=storage_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FrozenTrial(number=2, state=TrialState.COMPLETE, values=[0.05471425876021385, 0.9876206785413549, 0.06232230276080042], datetime_start=datetime.datetime(2024, 4, 9, 22, 5, 30, 359980), datetime_complete=datetime.datetime(2024, 4, 10, 0, 26, 13, 613859), params={'lora_r_value': 4, 'lora_alpha_value': 16, 'warmup_steps': 1033, 'num_train_epochs': 2, 'learning_rate': 1.3399549522183029e-05}, user_attrs={}, system_attrs={'nsga3:generation': 0}, intermediate_values={}, distributions={'lora_r_value': IntDistribution(high=16, log=False, low=4, step=1), 'lora_alpha_value': IntDistribution(high=16, log=False, low=4, step=1), 'warmup_steps': IntDistribution(high=1200, log=False, low=200, step=1), 'num_train_epochs': IntDistribution(high=5, log=False, low=1, step=1), 'learning_rate': FloatDistribution(high=5e-05, log=True, low=1e-05, step=None)}, trial_id=3, value=None),\n",
       " FrozenTrial(number=6, state=TrialState.COMPLETE, values=[0.03727734833955765, 0.9903956177128287, 0.05231322970776793], datetime_start=datetime.datetime(2024, 4, 10, 9, 25, 29, 486916), datetime_complete=datetime.datetime(2024, 4, 10, 14, 44, 53, 408059), params={'lora_r_value': 11, 'lora_alpha_value': 6, 'warmup_steps': 265, 'num_train_epochs': 5, 'learning_rate': 4.730944206898513e-05}, user_attrs={}, system_attrs={'nsga3:generation': 0}, intermediate_values={}, distributions={'lora_r_value': IntDistribution(high=16, log=False, low=4, step=1), 'lora_alpha_value': IntDistribution(high=16, log=False, low=4, step=1), 'warmup_steps': IntDistribution(high=1200, log=False, low=200, step=1), 'num_train_epochs': IntDistribution(high=5, log=False, low=1, step=1), 'learning_rate': FloatDistribution(high=5e-05, log=True, low=1e-05, step=None)}, trial_id=7, value=None),\n",
       " FrozenTrial(number=39, state=TrialState.COMPLETE, values=[0.03779778257012367, 0.990521759172176, 0.058879328394667835], datetime_start=datetime.datetime(2024, 4, 14, 20, 5, 56, 711789), datetime_complete=datetime.datetime(2024, 4, 15, 1, 24, 41, 843686), params={'lora_r_value': 8, 'lora_alpha_value': 13, 'warmup_steps': 1098, 'num_train_epochs': 5, 'learning_rate': 3.50840413075193e-05}, user_attrs={}, system_attrs={'nsga3:generation': 0}, intermediate_values={}, distributions={'lora_r_value': IntDistribution(high=16, log=False, low=4, step=1), 'lora_alpha_value': IntDistribution(high=16, log=False, low=4, step=1), 'warmup_steps': IntDistribution(high=1200, log=False, low=200, step=1), 'num_train_epochs': IntDistribution(high=5, log=False, low=1, step=1), 'learning_rate': FloatDistribution(high=5e-05, log=True, low=1e-05, step=None)}, trial_id=40, value=None),\n",
       " FrozenTrial(number=42, state=TrialState.COMPLETE, values=[0.044769879430532455, 0.98913161645429, 0.05982990518161062], datetime_start=datetime.datetime(2024, 4, 15, 8, 7, 16, 840544), datetime_complete=datetime.datetime(2024, 4, 15, 10, 28, 49, 325439), params={'lora_r_value': 11, 'lora_alpha_value': 12, 'warmup_steps': 852, 'num_train_epochs': 2, 'learning_rate': 3.146240472594082e-05}, user_attrs={}, system_attrs={'nsga3:generation': 0}, intermediate_values={}, distributions={'lora_r_value': IntDistribution(high=16, log=False, low=4, step=1), 'lora_alpha_value': IntDistribution(high=16, log=False, low=4, step=1), 'warmup_steps': IntDistribution(high=1200, log=False, low=200, step=1), 'num_train_epochs': IntDistribution(high=5, log=False, low=1, step=1), 'learning_rate': FloatDistribution(high=5e-05, log=True, low=1e-05, step=None)}, trial_id=43, value=None),\n",
       " FrozenTrial(number=45, state=TrialState.COMPLETE, values=[0.037969060242176056, 0.9906828678678538, 0.05328404651259251], datetime_start=datetime.datetime(2024, 4, 15, 17, 12, 15, 606164), datetime_complete=datetime.datetime(2024, 4, 15, 22, 38, 55, 91010), params={'lora_r_value': 7, 'lora_alpha_value': 16, 'warmup_steps': 593, 'num_train_epochs': 5, 'learning_rate': 2.761512212270469e-05}, user_attrs={}, system_attrs={'nsga3:generation': 0}, intermediate_values={}, distributions={'lora_r_value': IntDistribution(high=16, log=False, low=4, step=1), 'lora_alpha_value': IntDistribution(high=16, log=False, low=4, step=1), 'warmup_steps': IntDistribution(high=1200, log=False, low=200, step=1), 'num_train_epochs': IntDistribution(high=5, log=False, low=1, step=1), 'learning_rate': FloatDistribution(high=5e-05, log=True, low=1e-05, step=None)}, trial_id=46, value=None),\n",
       " FrozenTrial(number=61, state=TrialState.COMPLETE, values=[0.03760361671447754, 0.9903955472425768, 0.05335952221900954], datetime_start=datetime.datetime(2024, 4, 18, 10, 48, 36, 980826), datetime_complete=datetime.datetime(2024, 4, 18, 16, 7, 59, 130587), params={'lora_r_value': 7, 'lora_alpha_value': 12, 'warmup_steps': 941, 'num_train_epochs': 5, 'learning_rate': 3.228504412800339e-05}, user_attrs={}, system_attrs={'nsga3:generation': 1}, intermediate_values={}, distributions={'lora_r_value': IntDistribution(high=16, log=False, low=4, step=1), 'lora_alpha_value': IntDistribution(high=16, log=False, low=4, step=1), 'warmup_steps': IntDistribution(high=1200, log=False, low=200, step=1), 'num_train_epochs': IntDistribution(high=5, log=False, low=1, step=1), 'learning_rate': FloatDistribution(high=5e-05, log=True, low=1e-05, step=None)}, trial_id=62, value=None),\n",
       " FrozenTrial(number=102, state=TrialState.COMPLETE, values=[0.0320228673517704, 0.9913317036199346, 0.04818554056830126], datetime_start=datetime.datetime(2024, 4, 26, 11, 26, 52, 445441), datetime_complete=datetime.datetime(2024, 4, 26, 19, 43, 0, 668788), params={'lora_r_value': 6, 'lora_alpha_value': 16, 'warmup_steps': 917, 'num_train_epochs': 8, 'learning_rate': 3.7397629638193994e-05}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'lora_r_value': IntDistribution(high=16, log=False, low=4, step=1), 'lora_alpha_value': IntDistribution(high=16, log=False, low=4, step=1), 'warmup_steps': IntDistribution(high=1200, log=False, low=200, step=1), 'num_train_epochs': IntDistribution(high=10, log=False, low=5, step=1), 'learning_rate': FloatDistribution(high=5e-05, log=True, low=1e-05, step=None)}, trial_id=103, value=None),\n",
       " FrozenTrial(number=103, state=TrialState.COMPLETE, values=[0.03282594680786133, 0.9915014466831178, 0.05016064985909482], datetime_start=datetime.datetime(2024, 4, 26, 19, 43, 0, 717992), datetime_complete=datetime.datetime(2024, 4, 27, 3, 59, 11, 130874), params={'lora_r_value': 6, 'lora_alpha_value': 16, 'warmup_steps': 931, 'num_train_epochs': 8, 'learning_rate': 3.7041045196638695e-05}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'lora_r_value': IntDistribution(high=16, log=False, low=4, step=1), 'lora_alpha_value': IntDistribution(high=16, log=False, low=4, step=1), 'warmup_steps': IntDistribution(high=1200, log=False, low=200, step=1), 'num_train_epochs': IntDistribution(high=10, log=False, low=5, step=1), 'learning_rate': FloatDistribution(high=5e-05, log=True, low=1e-05, step=None)}, trial_id=104, value=None),\n",
       " FrozenTrial(number=104, state=TrialState.COMPLETE, values=[0.032357241958379745, 0.9914752054131631, 0.04809128362562369], datetime_start=datetime.datetime(2024, 4, 27, 3, 59, 11, 164140), datetime_complete=datetime.datetime(2024, 4, 27, 12, 15, 45, 348004), params={'lora_r_value': 6, 'lora_alpha_value': 16, 'warmup_steps': 905, 'num_train_epochs': 8, 'learning_rate': 3.645924341657196e-05}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'lora_r_value': IntDistribution(high=16, log=False, low=4, step=1), 'lora_alpha_value': IntDistribution(high=16, log=False, low=4, step=1), 'warmup_steps': IntDistribution(high=1200, log=False, low=200, step=1), 'num_train_epochs': IntDistribution(high=10, log=False, low=5, step=1), 'learning_rate': FloatDistribution(high=5e-05, log=True, low=1e-05, step=None)}, trial_id=105, value=None),\n",
       " FrozenTrial(number=107, state=TrialState.COMPLETE, values=[0.03271165117621422, 0.9915364748149553, 0.049055328803974944], datetime_start=datetime.datetime(2024, 4, 27, 21, 57, 23, 699838), datetime_complete=datetime.datetime(2024, 4, 28, 6, 13, 32, 320585), params={'lora_r_value': 6, 'lora_alpha_value': 23, 'warmup_steps': 1071, 'num_train_epochs': 8, 'learning_rate': 2.9534181738850096e-05}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'lora_r_value': IntDistribution(high=16, log=False, low=4, step=1), 'lora_alpha_value': IntDistribution(high=32, log=False, low=4, step=1), 'warmup_steps': IntDistribution(high=1200, log=False, low=200, step=1), 'num_train_epochs': IntDistribution(high=12, log=False, low=5, step=1), 'learning_rate': FloatDistribution(high=5e-05, log=True, low=1e-05, step=None)}, trial_id=108, value=None),\n",
       " FrozenTrial(number=108, state=TrialState.COMPLETE, values=[0.03192207217216492, 0.991245400790762, 0.04879557670799476], datetime_start=datetime.datetime(2024, 4, 28, 6, 13, 32, 354219), datetime_complete=datetime.datetime(2024, 4, 28, 14, 29, 49, 353852), params={'lora_r_value': 5, 'lora_alpha_value': 18, 'warmup_steps': 911, 'num_train_epochs': 8, 'learning_rate': 3.72998785772785e-05}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'lora_r_value': IntDistribution(high=16, log=False, low=4, step=1), 'lora_alpha_value': IntDistribution(high=32, log=False, low=4, step=1), 'warmup_steps': IntDistribution(high=1200, log=False, low=200, step=1), 'num_train_epochs': IntDistribution(high=12, log=False, low=5, step=1), 'learning_rate': FloatDistribution(high=5e-05, log=True, low=1e-05, step=None)}, trial_id=109, value=None)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get best trails from study\n",
    "study.best_trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Study to Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>values_0</th>\n",
       "      <th>values_1</th>\n",
       "      <th>values_2</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_learning_rate</th>\n",
       "      <th>params_lora_alpha_value</th>\n",
       "      <th>params_lora_r_value</th>\n",
       "      <th>params_num_train_epochs</th>\n",
       "      <th>params_warmup_steps</th>\n",
       "      <th>system_attrs_nsga3:generation</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.047838</td>\n",
       "      <td>0.988572</td>\n",
       "      <td>0.049686</td>\n",
       "      <td>2024-04-09 14:26:04.516440</td>\n",
       "      <td>2024-04-09 17:44:53.662875</td>\n",
       "      <td>0 days 03:18:49.146435</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.043519</td>\n",
       "      <td>0.989534</td>\n",
       "      <td>0.047869</td>\n",
       "      <td>2024-04-09 17:44:53.704285</td>\n",
       "      <td>2024-04-09 22:05:30.327186</td>\n",
       "      <td>0 days 04:20:36.622901</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.054714</td>\n",
       "      <td>0.987621</td>\n",
       "      <td>0.062322</td>\n",
       "      <td>2024-04-09 22:05:30.359980</td>\n",
       "      <td>2024-04-10 00:26:13.613859</td>\n",
       "      <td>0 days 02:20:43.253879</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.051484</td>\n",
       "      <td>0.987701</td>\n",
       "      <td>0.051460</td>\n",
       "      <td>2024-04-10 00:26:13.647323</td>\n",
       "      <td>2024-04-10 03:45:18.480474</td>\n",
       "      <td>0 days 03:19:04.833151</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.055351</td>\n",
       "      <td>0.986182</td>\n",
       "      <td>0.051536</td>\n",
       "      <td>2024-04-10 03:45:18.521370</td>\n",
       "      <td>2024-04-10 06:06:12.163497</td>\n",
       "      <td>0 days 02:20:53.642127</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number  values_0  values_1  values_2             datetime_start  \\\n",
       "0       0  0.047838  0.988572  0.049686 2024-04-09 14:26:04.516440   \n",
       "1       1  0.043519  0.989534  0.047869 2024-04-09 17:44:53.704285   \n",
       "2       2  0.054714  0.987621  0.062322 2024-04-09 22:05:30.359980   \n",
       "3       3  0.051484  0.987701  0.051460 2024-04-10 00:26:13.647323   \n",
       "4       4  0.055351  0.986182  0.051536 2024-04-10 03:45:18.521370   \n",
       "\n",
       "           datetime_complete               duration  params_learning_rate  \\\n",
       "0 2024-04-09 17:44:53.662875 0 days 03:18:49.146435              0.000013   \n",
       "1 2024-04-09 22:05:30.327186 0 days 04:20:36.622901              0.000031   \n",
       "2 2024-04-10 00:26:13.613859 0 days 02:20:43.253879              0.000013   \n",
       "3 2024-04-10 03:45:18.480474 0 days 03:19:04.833151              0.000016   \n",
       "4 2024-04-10 06:06:12.163497 0 days 02:20:53.642127              0.000021   \n",
       "\n",
       "   params_lora_alpha_value  params_lora_r_value  params_num_train_epochs  \\\n",
       "0                       16                    8                        3   \n",
       "1                        4                    6                        4   \n",
       "2                       16                    4                        2   \n",
       "3                        7                    6                        3   \n",
       "4                        5                   11                        2   \n",
       "\n",
       "   params_warmup_steps  system_attrs_nsga3:generation     state  \n",
       "0                  932                            0.0  COMPLETE  \n",
       "1                 1067                            0.0  COMPLETE  \n",
       "2                 1033                            0.0  COMPLETE  \n",
       "3                  725                            0.0  COMPLETE  \n",
       "4                  492                            0.0  COMPLETE  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = study.trials_dataframe()\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
