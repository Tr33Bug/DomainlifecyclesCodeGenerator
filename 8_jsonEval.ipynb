{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "# !pip install pandas\n",
    "# !pip3 install torch torchvision torchaudio\n",
    "# !pip install ipywidgets\n",
    "# !pip install bitsandbytes\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define run name\n",
    "run_name = \"finalTraining_v1\"\n",
    "# run_name = \"MLPC-2048-StarCoderBase7B\"\n",
    "\n",
    "# define model for tokenizer\n",
    "model_name = \"codellama/CodeLlama-7b-hf\"\n",
    "# model_name = \"bigcode/starcoderbase-7b\"\n",
    "\n",
    "# dataset import folder\n",
    "export_folder = \"./dataset/\" + run_name + \"/\"\n",
    "\n",
    "# model save path\n",
    "model_save_path = \"./models/\" + run_name + \"/\"\n",
    "\n",
    "# model checkpoint path\n",
    "model_checkpoint_path = \"./checkpoints/\" + run_name + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ffb9d45da64691b92a0a5632106a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Test loading model and inference with that model\n",
    "\n",
    "# load quantization config for 4bit quantization -> must be same as training\n",
    "quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "# load model from model_save_path with quantization config\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_save_path, quantization_config=quantization_config, low_cpu_mem_usage=True)\n",
    "\n",
    "# optional: load model untrained\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, low_cpu_mem_usage=True)\n",
    "\n",
    "# optional: load model unquantized and untrained\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True)\n",
    "\n",
    "# optional: load model from checkpoint\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"./output/bigRun/checkpoint-1000\", quantization_config=quantization_config, low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# add pad token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateJSON(model, tokenizer, prompt=\"<START> {\", temperature=0.1, max_length=300, end_token=\"<END>\", pad_token=\"[PAD]\"):\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    end_token_id = tokenizer.encode(end_token, add_special_tokens=False)[0]\n",
    "    pad_token_id = tokenizer.encode(pad_token, add_special_tokens=True)[0]\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    output = model.generate(input_ids, pad_token_id=pad_token_id, eos_token_id=end_token_id, temperature=temperature, max_length=max_length)\n",
    "\n",
    "    output_dict = {}\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    output_dict[\"generated_text\"] = generated_text\n",
    "    \n",
    "    if output.shape[1] == max_length:\n",
    "        # print(\"The output was cut off because it reached the maximum length.\")\n",
    "        output_dict[\"status\"] = \"max_lim\"\n",
    "        return output_dict\n",
    "    elif output[0][-1] == end_token_id:\n",
    "        # print(\"The output ended with the end-of-sequence token.\")\n",
    "        output_dict[\"status\"] = \"eos_token\"\n",
    "        return generated_text\n",
    "    else:\n",
    "        print(\"The output ended for an unknown reason.\")\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dict = generateJSON(model, tokenizer, prompt=\"<START> {\", temperature=0.1, max_gen_length=10, end_token=\"<END>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dict['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<START>{ \\\"@class\\\" : \\\"nitrox.dlc.mirror.model.FieldModel\\\",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated_json.json', 'w') as f:\n",
    "        cleaned_prompt = prompt.replace(\"<START>\", \"\")\n",
    "        f.write(cleaned_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henrikwiegand/thesis/thesis-localllm-codetuning/FixCUDAShit/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/henrikwiegand/thesis/thesis-localllm-codetuning/FixCUDAShit/lib/python3.10/site-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_dict = generateJSON(model, tokenizer, prompt=prompt, temperature=0.1, max_length=300, end_token=\"<END>\", pad_token=\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'max_lim'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START>{ \"@class\" : \"nitrox.dlc.mirror.model.FieldModel\", \"name\" : \"VAR_name\", \"type\" : {\"@class\" : \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\" : \"VAR_typeName\", \"domainType\" : \"NON_DOMAIN\", \"assertions\" : [{\"@class\" : \"nitrox.dlc.mirror.model.AssertionModel\", \"assertionType\" : \"isNotNull\", \"param1\" : null, \"param2\" : null, \"message\" : \"{jakarta.validation.constraints.NotNull.message}\"}], \"hasOptionalContainer\" : false, \"hasCollectionContainer\" : false, \"hasListContainer\" : false, \"hasSetContainer\" : false, \"hasStreamContainer\" : false, \"containerTypeName\" : \"VAR_containerTypeName\", \"containerAssertions\" : []}, \"accessLevel\" : \"PACKAGE\", \"declaredByTypeName\" : \"VAR_declaredByTypeName\", \"modifiable\" : true, \"publicReadable\" : true, \"publicWriteable\" : false, \"static\" : false}, {\"@class\" : \"nitrox.dlc.mirror.model.FieldModel\", \"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text = output_dict['generated_text']\n",
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"name\" : \"VAR_name\", \"type\" : {\"@class\" : \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\" : \"VAR_typeName\", \"domainType\" : \"NON_DOMAIN\", \"assertions\" : [{\"@class\" : \"nitrox.dlc.mirror.model.AssertionModel\", \"assertionType\" : \"isNotNull\", \"param1\" : null, \"param2\" : null, \"message\" : \"{jakarta.validation.constraints.NotNull.message}\"}], \"hasOptionalContainer\" : false, \"hasCollectionContainer\" : false, \"hasListContainer\" : false, \"hasSetContainer\" : false, \"hasStreamContainer\" : false, \"containerTypeName\" : \"VAR_containerTypeName\", \"containerAssertions\" : []}, \"accessLevel\" : \"PACKAGE\", \"declaredByTypeName\" : \"VAR_declaredByTypeName\", \"modifiable\" : true, \"publicReadable\" : true, \"publicWriteable\" : false, \"static\" : false}, {\"@class\" : \"nitrox.dlc.mirror.model.FieldModel\", \"'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if gen_text.startswith(prompt):\n",
    "        result = gen_text[len(prompt):]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated_json.json', 'a') as f:\n",
    "        f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': false, \"containerTypeName\" : \"VAR_containerTypeName\", \"containerAssertions\" : []}, \"accessLevel\" : \"PACKAGE\", \"declaredByTypeName\" : \"VAR_declaredByTypeName\", \"modifiable\" : true, \"publicReadable\" : true, \"publicWriteable\" : false, \"static\" : false}, {\"@class\" : \"nitrox.dlc.mirror.model.FieldModel\", \"'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = result.split()\n",
    "if len(words) >= 30:\n",
    "        prompt = words[-30:]\n",
    "        prompt = ' '.join(prompt)\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean VRAM\n",
    "del output_dict\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = generateJSON(model, tokenizer, prompt=prompt, temperature=0.1, max_length=300, end_token=\"<END>\", pad_token=\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'max_lim'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': false, \"containerTypeName\" : \"VAR_containerTypeName\", \"containerAssertions\" : []}, \"accessLevel\" : \"PACKAGE\", \"declaredByTypeName\" : \"VAR_declaredByTypeName\", \"modifiable\" : true, \"publicReadable\" : true, \"publicWriteable\" : false, \"static\" : false}, {\"@class\" : \"nitrox.dlc.mirror.model.FieldModel\", \"name\" : \"VAR_name\", \"type\" : {\"@class\" : \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\" : \"VAR_typeName\", \"domainType\" : \"NON_DOMAIN\", \"assertions\" : [], \"hasOptionalContainer\" : false, \"hasCollectionContainer\" : false, \"hasListContainer\" : false, \"hasSetContainer\" : false, \"hasStreamContainer\" : false, \"containerTypeName\" : \"VAR_containerTypeName\", \"containerAssertions\" : []}, \"accessLevel\" : \"PACKAGE\", \"declaredByTypeName\" : \"VAR_declaredByTypeName\", \"modifiable\" : true, \"publicReadable\" : true, \"publicWriteable\" : false, \"static\" : false}, {\"@class\" : \"nitrox'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text = output_dict['generated_text']\n",
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name\" : \"VAR_name\", \"type\" : {\"@class\" : \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\" : \"VAR_typeName\", \"domainType\" : \"NON_DOMAIN\", \"assertions\" : [], \"hasOptionalContainer\" : false, \"hasCollectionContainer\" : false, \"hasListContainer\" : false, \"hasSetContainer\" : false, \"hasStreamContainer\" : false, \"containerTypeName\" : \"VAR_containerTypeName\", \"containerAssertions\" : []}, \"accessLevel\" : \"PACKAGE\", \"declaredByTypeName\" : \"VAR_declaredByTypeName\", \"modifiable\" : true, \"publicReadable\" : true, \"publicWriteable\" : false, \"static\" : false}, {\"@class\" : \"nitrox'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if gen_text.startswith(prompt):\n",
    "        result = gen_text[len(prompt):]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated_json.json', 'a') as f:\n",
    "        f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN: 0\n",
      "Start new run with prompt:\n",
      "<START> {\"nitrox.dlc.mirror.model.FieldModel\",\n",
      "\n",
      "\n",
      "Generating ⏳\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henrikwiegand/thesis/thesis-localllm-codetuning/FixCUDAShit/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/henrikwiegand/thesis/thesis-localllm-codetuning/FixCUDAShit/lib/python3.10/site-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generated ✅\n",
      "max_lim\n",
      "🤖 I Generated this text: <START> {\"nitrox.dlc.mirror.model.FieldModel\", \"name\": \"VAR_name\", \"type\": {\"@class\": \"nitrox.dlc.mirror.model.AssertedContainableTypeModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"NON_DOMAIN\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": []}, \"accessLevel\": \"PACKAGE\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"modifiable\": true, \"publicReadable\": true, \"publicWriteable\": false, \"static\": false}, \"inheritanceHierarchyTypeNames\": \"VAR_inheritanceHierarchyTypeNames\", \"allInterfaceTypeNames\": \"VAR_allInterfaceTypeNames\"}<END>}, {\"@class\": \"nitrox.dlc.mirror.model.ValueObjectModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"VALUE_OBJECT\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\":\n",
      "RUN: 1\n",
      "Start new run with prompt:\n",
      "\"PACKAGE\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"modifiable\": true, \"publicReadable\": true, \"publicWriteable\": false, \"static\": false}, \"inheritanceHierarchyTypeNames\": \"VAR_inheritanceHierarchyTypeNames\", \"allInterfaceTypeNames\": \"VAR_allInterfaceTypeNames\"}<END>}, {\"@class\": \"nitrox.dlc.mirror.model.ValueObjectModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"VALUE_OBJECT\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\":\n",
      "\n",
      "\n",
      "Generating ⏳\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henrikwiegand/thesis/thesis-localllm-codetuning/FixCUDAShit/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/henrikwiegand/thesis/thesis-localllm-codetuning/FixCUDAShit/lib/python3.10/site-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generated ✅\n",
      "max_lim\n",
      "🤖 I Generated this text: \"PACKAGE\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"modifiable\": true, \"publicReadable\": true, \"publicWriteable\": false, \"static\": false}, \"inheritanceHierarchyTypeNames\": \"VAR_inheritanceHierarchyTypeNames\", \"allInterfaceTypeNames\": \"VAR_allInterfaceTypeNames\"}<END>}, {\"@class\": \"nitrox.dlc.mirror.model.ValueObjectModel\", \"typeName\": \"VAR_typeName\", \"domainType\": \"VALUE_OBJECT\", \"assertions\": [], \"hasOptionalContainer\": false, \"hasCollectionContainer\": false, \"hasListContainer\": false, \"hasSetContainer\": false, \"hasStreamContainer\": false, \"containerTypeName\": \"VAR_containerTypeName\", \"containerAssertions\": [], \"static\": false, \"accessLevel\": \"PRIVATE\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"methods\": [{\"@class\": \"nitrox.dlc.mirror.model.MethodModel\", \"name\": \"VAR_name\", \"declaredByTypeName\": \"VAR_declaredByTypeName\", \"accessLevel\": \"PUBLIC\", \"parameters\": [{\"@class\": \"nitrox.dlc.mirror.model.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🤖 I Generated this text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgen_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# save generated text without prompt:\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mgen_text\u001b[49m\u001b[38;5;241m.\u001b[39mstartswith(prompt):\n\u001b[1;32m     24\u001b[0m     result \u001b[38;5;241m=\u001b[39m gen_text[\u001b[38;5;28mlen\u001b[39m(prompt):]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🤖 I Generated this text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgen_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# save generated text without prompt:\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mgen_text\u001b[49m\u001b[38;5;241m.\u001b[39mstartswith(prompt):\n\u001b[1;32m     24\u001b[0m     result \u001b[38;5;241m=\u001b[39m gen_text[\u001b[38;5;28mlen\u001b[39m(prompt):]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/thesis/thesis-localllm-codetuning/FixCUDAShit/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/thesis-localllm-codetuning/FixCUDAShit/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompt = \"<START>{ \\\"@class\\\" : \\\"nitrox.dlc.mirror.model.FieldModel\\\",\"\n",
    "for i in range(30):\n",
    "    print(f\"RUN: {i}\\nStart new run with prompt:\\n{prompt}\")\n",
    "\n",
    "    print(\"\\n\\nGenerating ⏳\")\n",
    "    output_dict = generateJSON(model, tokenizer, prompt=prompt, temperature=0.1, max_length=300, end_token=\"<END>\", pad_token=\"[PAD]\")\n",
    "    print(\"\\n\\nGenerated ✅\")\n",
    "    \n",
    "    if output_dict['status'] != \"eos_token\":\n",
    "        print(output_dict['status'])\n",
    "    else:\n",
    "        print(\"This is the end! 🥳:\")\n",
    "        print(output_dict['generated_text'])\n",
    "        break\n",
    "\n",
    "    \n",
    "    # generated Text\n",
    "    gen_text = output_dict['generated_text']\n",
    "\n",
    "    print(f\"🤖 I Generated this text: {gen_text}\")\n",
    "    \n",
    "    # save generated text without prompt:\n",
    "    if gen_text.startswith(prompt):\n",
    "        result = gen_text[len(prompt):]\n",
    "    else:\n",
    "        print(\"ERROR: The Prompt is not in the generated text!\")\n",
    "        break\n",
    "        \n",
    "    \n",
    "    with open('generated_text.txt', 'a') as f:\n",
    "        f.write(result)\n",
    "    \n",
    "\n",
    "    # create prompt for next run with last 30 characters\n",
    "    words = gen_text.split()\n",
    "\n",
    "    if len(words) >= 30:\n",
    "        prompt = words[-30:]\n",
    "        prompt = ' '.join(prompt)\n",
    "\n",
    "    else:\n",
    "        print(f\"generated Text is not long enough! The length ist: {len(words)}\\n\")\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    del output_dict\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Try to parse the generated text as JSON\n",
    "try:\n",
    "    parsed_json = json.loads(generated_text)\n",
    "    print(\"The generated text is valid JSON.\")\n",
    "\n",
    "    # Format the JSON\n",
    "    formatted_json = json.dumps(parsed_json, indent=4)\n",
    "\n",
    "    # save formatted json to file\n",
    "    with open(\"generated_json.json\", \"w\") as f:\n",
    "        f.write(formatted_json)\n",
    "\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"The generated text is not valid JSON. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
