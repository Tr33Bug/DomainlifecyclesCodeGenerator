{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define run name\n",
    "run_name = \"A6000_StartEnd_without_po\"\n",
    "\n",
    "# define model for tokenizer\n",
    "model_name = \"codellama/CodeLlama-7b-hf\"\n",
    "\n",
    "# dataset import folder\n",
    "export_folder = \"./dataset/\" + run_name + \"/\"\n",
    "\n",
    "# model save path\n",
    "model_save_path = \"./models/\" + run_name + \"/\"\n",
    "\n",
    "# model checkpoint path\n",
    "model_checkpoint_path = \"./checkpoints/\" + run_name + \"/\"\n",
    "\n",
    "# Tensorboard folder\n",
    "tensorboard_logdir = \"./runs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78f31ea9d974e668344cfd5b6469e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Test loading model and inference with that model\n",
    "\n",
    "# load quantization config for 4bit quantization -> must be same as training\n",
    "quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "# load model from model_save_path with quantization config\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_save_path, quantization_config=quantization_config, low_cpu_mem_usage=True)\n",
    "\n",
    "# optional: load model from checkpoint\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"./output/bigRun/checkpoint-1000\", quantization_config=quantization_config, low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# add pad token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenized test dataset\n",
    "test_dataset = datasets.load_from_disk(export_folder + \"test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training parameters\n",
    "\n",
    "# set number of epochs\n",
    "num_train_epochs = 3\n",
    "\n",
    "# set batch size per device\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# set number of gradient accumulation steps -> number of updates steps to accumulate before performing a backward/update pass\n",
    "gradient_accumulation_steps=5\n",
    "\n",
    "# create model checkpoint every x steps\n",
    "save_steps=20\n",
    "\n",
    "# Keep keep last x checkpoints\n",
    "save_total_limit=5\n",
    "\n",
    "# Enable mixed precision training -> hugh enabler for low VRAM training\n",
    "fp16=True\n",
    "\n",
    "# Log every x steps\n",
    "logging_steps=50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING HYPERPARAMETERS ###\n",
    "\n",
    "## Lora\n",
    "# lora rank\n",
    "lora_r_value = 8\n",
    "\n",
    "# lora alpha\n",
    "lora_alpha_value = 16\n",
    "\n",
    "# dropout for lora weights\n",
    "lora_dropout = 0.05\n",
    "\n",
    "## trainer\n",
    "# Number of warmup steps for learning rate scheduler\n",
    "warmup_steps=800\n",
    "\n",
    "# Learning rate\n",
    "learning_rate=2e-5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)  # Set mlm=False for causal language modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=model_checkpoint_path,  # Output directory for model predictions and checkpoints\n",
    "    overwrite_output_dir=True,  # Overwrite existing output\n",
    "    num_train_epochs=num_train_epochs, # Number of training epochs\n",
    "    per_device_train_batch_size=per_device_train_batch_size,  # Batch size per device during training\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # Number of updates steps to accumulate before performing a backward/update pass\n",
    "    save_steps=save_steps,  # Create model checkpoint every x steps\n",
    "    save_total_limit=save_total_limit,  # Keep keep last x checkpoints\n",
    "    fp16=True,  # Enable mixed precision training -> hugh enabler for low VRAM training\n",
    "    logging_dir=tensorboard_logdir,  # Directory for storing logs\n",
    "    logging_steps=logging_steps,  # Log every x steps\n",
    "    warmup_steps=warmup_steps,  # Number of warmup steps for learning rate scheduler\n",
    "    learning_rate=learning_rate,  # Learning rate\n",
    "    evaluation_strategy=\"steps\",  # Evaluate every `logging_steps`\n",
    "    eval_steps=1,  # Evaluate every x steps\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_checkpointing=True,\n",
    "    debug=True,\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import nltk\n",
    "from codebleu import calc_codebleu\n",
    "from rouge import Rouge\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    torch.cuda.empty_cache()\n",
    "    logits, labels = eval_pred\n",
    "    # print(\"Min token id:\", min(labels.flatten()))\n",
    "    # print(\"Max token id:\", max(labels.flatten()))\n",
    "\n",
    "    mask = labels!=-100 # we dont need to calculate loss for -100 tokens -> padding tokens\n",
    "    logits, labels = logits[mask], labels[mask]\n",
    "\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    if isinstance(labels, np.ndarray):\n",
    "        labels = torch.from_numpy(labels)\n",
    "\n",
    "    # Ensure the tensors are on the same device and in the correct format\n",
    "    logits = logits.to(labels.device).view(-1, logits.size(-1))\n",
    "    labels = labels.view(-1)\n",
    "\n",
    "    \n",
    "    ### Calculate cross entropy\n",
    "    loss = F.cross_entropy(logits, labels.long())\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity = torch.exp(loss)\n",
    "\n",
    "    # print(\"Perplexity:\", perplexity.item())\n",
    "\n",
    "    ### Calculate BLEU score\n",
    "\n",
    "    smoothing_function = nltk.translate.bleu_score.SmoothingFunction().method1\n",
    "\n",
    "    references = [tokenizer.decode(labels.tolist()).split()]\n",
    "    # print(\"References:\", references)\n",
    "    candidates = [tokenizer.decode(logits.argmax(dim=-1).tolist()).split()]\n",
    "    # print(\"Candidates:\", candidates)\n",
    "    bleu_score = corpus_bleu(references, candidates, weights=(0.5, 0.5), smoothing_function=smoothing_function)\n",
    "\n",
    "    # print(\"BLEU Score:\", bleu_score)\n",
    "\n",
    "    # Calculate codeBLEU score\n",
    "    # codebleu_score = calc_codebleu(references, candidates, lang=\"JSON\")\n",
    "    # print(codebleu_score)\n",
    "    \n",
    "    # Calculate ROUGE score\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(' '.join(candidates[0]), ' '.join(references[0]))\n",
    "\n",
    "    # return {\"perplexity\": perplexity.item(), \"bleu_score\": bleu_score, \"codebleu_score\": codebleu_score, \"rouge_score\": scores}\n",
    "    # return {\"perplexity\": perplexity.item(), \"bleu_score\": bleu_score, \"rouge_score_1_f1\": scores[0][\"rouge-1\"][\"f\"], \"rouge_score_1_p\": scores[0][\"rouge-1\"][\"p\"], \"rouge_score_1_r\": scores[0][\"rouge-1\"][\"r\"], \"rouge_score_2_f1\": scores[0][\"rouge-2\"][\"f\"], \"rouge_score_2_p\": scores[0][\"rouge-2\"][\"p\"], \"rouge_score_2_r\": scores[0][\"rouge-2\"][\"r\"], \"rouge_score_l_f1\": scores[0][\"rouge-l\"][\"f\"], \"rouge_score_l_p\": scores[0][\"rouge-l\"][\"p\"], \"rouge_score_l_r\": scores[0][\"rouge-l\"][\"r\"]} \n",
    "    return {\"perplexity\": perplexity.item(), \"bleu_score\": bleu_score, \"rouge_score_1_f1\": scores[0][\"rouge-1\"][\"f\"], \"rouge_score_2_f1\": scores[0][\"rouge-2\"][\"f\"], \"rouge_score_l_f1\": scores[0][\"rouge-l\"][\"f\"]}\n",
    "    # return {\"perplexity\": perplexity.item(), \"bleu_score\": bleu_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'bool' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[43meval_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/local_lllm_tuning_without_po/lib/python3.11/site-packages/transformers/trainer.py:3389\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3378\u001b[0m output\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m   3379\u001b[0m     speed_metrics(\n\u001b[1;32m   3380\u001b[0m         metric_key_prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3384\u001b[0m     )\n\u001b[1;32m   3385\u001b[0m )\n\u001b[1;32m   3387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(output\u001b[38;5;241m.\u001b[39mmetrics)\n\u001b[0;32m-> 3389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mDebugOption\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTPU_METRICS_DEBUG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebug\u001b[49m:\n\u001b[1;32m   3390\u001b[0m     \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n\u001b[1;32m   3391\u001b[0m     xm\u001b[38;5;241m.\u001b[39mmaster_print(met\u001b[38;5;241m.\u001b[39mmetrics_report())\n\u001b[1;32m   3393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_evaluate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol, output\u001b[38;5;241m.\u001b[39mmetrics)\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'bool' is not iterable"
     ]
    }
   ],
   "source": [
    "eval_results = eval_trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meval_results\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_results' is not defined"
     ]
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ROUGE:\n",
    "\n",
    "ROUGE-Score 1 -> 1-gram wörter Übereinstimmung\n",
    "ROUGE-Score 2 -> 2-gram wörter Übereinstimmung\n",
    "ROUGE-Score l -> längste n-gram Übereinstimmung\n",
    "\n",
    "F1 Score: https://stephenallwright.com/interpret-f1-score/\n",
    "\n",
    "|Score|Interp.|\n",
    "|---|---|\n",
    "|>0.9 | Very good|\n",
    "|0.8 - 0.9\t|Good|\n",
    "|0.5 - 0.8\t|OK|\n",
    "|<0.5\t|Not good|\n",
    "\n",
    "### perplexity:\n",
    "0 - 100 gut\n",
    "Normalerweise <1000 \n",
    "\n",
    "### BLEU Score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
